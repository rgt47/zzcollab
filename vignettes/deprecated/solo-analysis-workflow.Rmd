---
title: "Solo Analysis Workflow: Complete Reproducible Data Science"
author: "ZZCOLLAB Team"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Solo Analysis Workflow: Complete Reproducible Data Science}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Solo Analysis Workflow

This vignette demonstrates how to use the zzcollab framework's
**analysis paradigm** to conduct reproducible data science research as a
solo developer. The analysis paradigm is specifically designed for data
analysis projects, providing specialized templates, packages, and
workflows for systematic exploratory data analysis, statistical
modeling, and reproducible reporting.

## Analysis Paradigm Overview

The **analysis paradigm** is one of zzcollab's three research paradigms,
optimized for data science workflows:

**Key Features:**

- **Systematic Templates**: Six professional workflow templates (01-06)
  for complete analysis pipelines
- **Specialized Packages**: Pre-configured with tidyverse, tidymodels,
  targets, plotly, DT, and analysis tools
- **Structured Directories**: Organized for `data/`, `analysis/`,
  `outputs/`, `scripts/`, and `reports/`
- **Reproducible Framework**: Built-in seed management, session
  logging, and dependency tracking
- **Publication Ready**: Professional visualizations and automated
  report generation

**Template Structure:**

- `01_exploratory_analysis.R` - Systematic EDA with quality assessment
- `02_statistical_modeling.R` - Modeling pipeline with tidymodels
- `03_model_validation.R` - Cross-validation and robustness testing
- `04_interactive_dashboard.Rmd` - Shiny dashboard for exploration
- `05_automated_report.Rmd` - Parameterized reports with multiple formats
- `example_analysis_functions.R` - Reusable utility functions

## Build Mode Overview

ZZCOLLAB provides three build modes that control the scope of R packages
installed in your Docker environment, allowing you to balance
functionality with build speed and container size:

**Build Mode Options:**

- **Fast Mode (-F)**: Essential packages for rapid development
  (9 packages, approximately 5-minute build time)
  - Core components: renv, here, usethis, devtools, testthat, knitr,
    rmarkdown, targets
  - Applications: Rapid prototyping, CI/CD workflows, minimal configurations
- **Standard Mode (-S)**: Balanced package selection for typical analytical
  workflows (17 packages, approximately 8-minute build time, default)
  - Includes Fast mode components plus tidyverse core: dplyr, ggplot2,
    tidyr, palmerpenguins, broom, janitor, DT, conflicted
  - Applications: Standard data analysis projects, recommended for individual
    researchers
- **Comprehensive Mode (-C)**: Extended ecosystem for complex analytical
  environments (47+ packages, approximately 15-minute build time)
  - Includes Standard mode components plus advanced tools: tidymodels,
    shiny, plotly, quarto, flexdashboard, survival, lme4
  - Applications: Complex multi-paradigm analyses, production environments

**Usage Examples:**
```bash
zzcollab -p my-project -F    # Fast mode
zzcollab -p my-project -S    # Standard mode (default)
zzcollab -p my-project -C    # Comprehensive mode
```

*For detailed build mode documentation, see [Build Modes Guide](../docs/BUILD_MODES.md)*

## Testing Framework Overview

ZZCOLLAB includes a comprehensive testing framework that ensures reproducibility and code quality throughout the analysis lifecycle:

**Testing Components:**

- **Unit Tests**: Individual function validation using testthat framework
- **Integration Tests**: End-to-end workflow validation from data input to
  analytical results
- **Dependency Validation**: Automated renv synchronization verification
- **Reproducibility Tests**: Session consistency and seed management validation

**Automated Testing Features:**

- **Test Templates**: Pre-configured test suites for analytical functions
- **Coverage Tracking**: Coverage targets exceeding 90% with automated reporting
- **CI/CD Integration**: GitHub Actions workflows for continuous validation
- **Quality Gates**: Automated verification prevents commits with test failures

**Testing Workflow:**

1. **Development Phase**: Function implementation with corresponding test
   development
2. **Local Validation**: `make docker-test` executes comprehensive test suite
3. **Pre-commit Verification**: Dependency validation prior to repository
   commits
4. **Continuous Integration**: Automated testing across multiple R versions
   upon repository updates

**Example Test Structure:**
```r
test_that("load_penguin_data works correctly", {
  penguins <- load_penguin_data("test_data.csv")
  expect_s3_class(penguins, "data.frame")
  expect_true(nrow(penguins) > 0)
})
```

*For complete testing documentation, see [Testing Guide](../docs/TESTING_GUIDE.md)*

## Configuration and Environment System Overview

ZZCOLLAB implements a hierarchical configuration system that manages Docker environments, R packages, and development settings through multiple configuration layers:

**Configuration Hierarchy** (priority order):

1. **Project config** (`./zzcollab.yaml`) - Team-specific settings
2. **User config** (`~/.zzcollab/config.yaml`) - Individual user defaults
3. **System config** (`/etc/zzcollab/config.yaml`) - Organization-level defaults
4. **Built-in defaults** - Framework fallback values

**Docker Environment System:**

- **Specialized Environments**: Fourteen distinct configurations ranging from
  lightweight Alpine distributions (approximately 200MB) to comprehensive
  environments (approximately 3.5GB)
- **Domain-Specific Variants**: Specialized configurations for bioinformatics,
  geospatial analysis, high-performance computing, and web applications
- **Interactive Management**: `./add_environment.sh` utility provides guided
  environment selection interface
- **Centralized Definitions**: Single source configuration library eliminates
  duplication across projects

**Configuration Domains:**

- **Build Settings**: Package selection modes, paradigm defaults, dotfiles
  integration
- **Docker Management**: Variant selection, platform compatibility, resource
  constraints
- **Collaboration Settings**: Team coordination protocols, version control
  integration, resource sharing preferences

**Quick Configuration:**
```bash
zzcollab --config init                    # Initialize config file
zzcollab --config set paradigm "analysis" # Set default paradigm
zzcollab --config set build-mode "standard" # Set default build mode
zzcollab --config list                    # View current settings
```

*For comprehensive configuration documentation, see [Configuration Guide](../docs/CONFIGURATION.md) and [Environment System Guide](../docs/VARIANTS.md)*

## Initial Setup (One-Time)

### 1. Install and Configure zzcollab

```bash
# Clone and install zzcollab framework
git clone https://github.com/rgt47/zzcollab.git
cd zzcollab && ./install.sh

# Initialize personal configuration (local development only)
zzcollab --config init
zzcollab --config set paradigm "analysis"       # Default to analysis paradigm
zzcollab --config set build-mode "standard"     # Balanced package set
zzcollab --config set dotfiles-dir "~/dotfiles" # Optional: your dotfiles

# Optional: Docker Hub configuration (only needed for sharing images)
# zzcollab --config set team-name "myname"      # Uncomment if you plan to share images
```

### 2. Verify Installation

```bash
# Verify zzcollab is properly installed
zzcollab --help
which zzcollab

# Check configuration
zzcollab --config list
```

## Project Creation and Setup

### Solo vs Team Development

**Solo Development (Recommended):**
- Work entirely locally with your own Docker containers
- No Docker Hub account or team configuration needed
- Perfect reproducibility for your individual work
- Easy to upgrade to team collaboration later

**Team Development (Optional):**
- Share Docker images via Docker Hub for team collaboration
- Requires `team-name` configuration and `-i` flag
- Enables multiple developers to use identical environments

### Create Analysis Project

```bash
# Method 1: Local development (recommended for solo work)
zzcollab -p penguin-analysis --github       # Local project, no Docker Hub needed

# Method 2: Explicit paradigm specification
zzcollab -p penguin-analysis -P analysis --github

# Method 3: Advanced - with custom team configuration (if sharing images later)
# zzcollab -i -p penguin-analysis -P analysis --github  # Creates shareable team images

# Method 4: Custom environment selection for specialized needs
mkdir penguin-analysis && cd penguin-analysis
zzcollab -p penguin-analysis -P analysis    # Local project setup
./add_environment.sh    # Select additional environments like modeling or alpine_minimal
```

**What this creates:**
- Complete R package structure with analysis-optimized DESCRIPTION
- Six professional analysis templates (01-06)
- Docker environment with tidyverse, tidymodels, and analysis packages
- GitHub repository with analysis-specific CI/CD workflows
- Systematic directory structure for reproducible data science

### Project Structure Created

```
penguin-analysis/
├── data/
│   ├── raw/              # Original, untouched data
│   └── processed/        # Cleaned, analysis-ready data
├── analysis/
│   ├── exploratory/      # EDA outputs and diagnostics
│   ├── modeling/         # Model objects and validation
│   └── validation/       # Cross-validation and robustness
├── outputs/
│   ├── figures/          # Publication-quality plots
│   └── tables/           # Summary statistics and results
├── reports/
│   └── dashboard/        # Interactive reports and dashboards
├── scripts/              # Analysis workflow templates (created by install)
│   ├── 01_exploratory_analysis.R      # Systematic EDA template
│   ├── 02_statistical_modeling.R      # tidymodels pipeline
│   ├── 03_model_validation.R          # Validation framework
│   ├── 04_interactive_dashboard.Rmd   # Shiny exploration
│   ├── 05_automated_report.Rmd        # Parameterized reporting
│   └── analysis_functions.R           # Utility functions
├── R/                    # Package functions for analysis
├── tests/testthat/       # Comprehensive testing framework
└── DESCRIPTION           # Analysis-optimized package metadata
```

## Daily Development Workflow

### 1. Start Development Environment

```bash
cd penguin-analysis

# Launch development environment (choose your interface)
make docker-zsh         # Command-line interface (most flexible)
make docker-rstudio     # RStudio Server at localhost:8787
make docker-r           # R console only
```

### 2. Development Cycle (Inside Container)

The analysis paradigm follows a systematic development pattern:

**Phase 1: Data Import and Exploration**

```bash
# Inside the Docker container with all packages pre-installed
# Work on exploratory data analysis
vim scripts/01_exploratory_analysis.R
```

**Phase 2: Statistical Modeling**

```bash
# Develop modeling pipeline
vim scripts/02_statistical_modeling.R
```

**Phase 3: Validation and Testing**

```bash
# Create analysis functions and tests
vim R/analysis_functions.R
vim tests/testthat/test-analysis_functions.R
```

**Phase 4: Execute Analysis**

```bash
# Run the complete analysis pipeline
R --vanilla < scripts/01_exploratory_analysis.R
R --vanilla < scripts/02_statistical_modeling.R

# Test your functions
R
devtools::load_all()
devtools::test()
quit()
```

### 3. Validation and Commit

```bash
# Exit development container
exit

# Back on host system - validate everything works
make docker-check-renv-fix    # Update dependency tracking
make docker-test             # Run tests in clean environment
make docker-render           # Ensure reports render correctly

# Commit your progress
git add .
git commit -m "Add systematic penguin analysis

- Implement exploratory data analysis with quality checks
- Create tidymodels statistical modeling pipeline
- Add comprehensive function testing framework
- Generate publication-quality visualizations
- All dependencies tracked, tests passing"

git push origin main
```

## Complete Practical Example: Palmer Penguins Analysis

Let's walk through a comprehensive analysis using the Palmer penguins dataset to demonstrate the complete workflow.

### Step 1: Project Setup and Data Import

```bash
# Create the project (local development)
zzcollab -p penguin-analysis -P analysis --github
cd penguin-analysis

# Start development environment
make docker-zsh
```

**Inside container - Set up data:**

```bash
# Create data structure and import Palmer penguins
mkdir -p data/raw data/processed

# In R, download and save Palmer penguins data
R
library(palmerpenguins)
library(readr)
data(penguins)
write_csv(penguins, "data/raw/penguins.csv")
quit()
```

### Step 2: Systematic Exploratory Data Analysis

Edit `scripts/01_exploratory_analysis.R`:

```r
# Exploratory Data Analysis - Palmer Penguins
# Project: penguin-analysis
# Focus: Systematic investigation of penguin bill relationships

# Load required packages (pre-installed in analysis paradigm)
library(here)
library(dplyr)
library(ggplot2)
library(skimr)
library(janitor)
library(readr)
library(plotly)
library(DT)

# Set reproducible seed
set.seed(42)

# Create session log for reproducibility
session_log <- list(
  script = "01_exploratory_analysis.R",
  start_time = Sys.time(),
  r_version = R.version.string,
  random_seed = 42,
  focus = "Palmer penguins bill dimension relationships"
)

# Set up paths
raw_data_dir <- here("data", "raw")
processed_data_dir <- here("data", "processed")
figures_dir <- here("outputs", "figures")

# Create output directories
for (dir in c(processed_data_dir, figures_dir)) {
  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)
}

message("Starting Palmer Penguins exploratory analysis...")

# =============================================================================
# DATA IMPORT AND INITIAL INSPECTION
# =============================================================================

message("\n1. DATA IMPORT AND INITIAL INSPECTION")

# Load Palmer penguins data
penguins_raw <- read_csv(file.path(raw_data_dir, "penguins.csv"))

# Basic data inspection
message("Dataset dimensions: ", nrow(penguins_raw), " rows × ", ncol(penguins_raw), " columns")
message("Variables: ", paste(names(penguins_raw), collapse = ", "))

# Data structure overview
glimpse(penguins_raw)

# =============================================================================
# DATA UNIT TESTING (Critical for Reproducible Analysis)
# =============================================================================

message("\n2. DATA UNIT TESTING")

# Load testing framework
library(testthat)

# Create data testing functions for reusability
# Note: Create R/data_testing_functions.R with reusable testing utilities
# source(here("R", "data_testing_functions.R"))  # Uncomment when file exists

# Raw data validation tests
message("Running unit tests on raw Palmer penguins data...")

# Test 1: Dataset structure and dimensions
test_that("Palmer penguins raw data has expected structure", {
  expect_true(is.data.frame(penguins_raw),
              info = "Raw data should be a data frame")
  expect_equal(nrow(penguins_raw), 344,
               info = "Palmer penguins should have 344 observations")
  expect_equal(ncol(penguins_raw), 8,
               info = "Palmer penguins should have 8 variables")
})

# Test 2: Required columns exist
expected_columns <- c("species", "island", "bill_length_mm", "bill_depth_mm",
                     "flipper_length_mm", "body_mass_g", "sex", "year")

test_that("Palmer penguins has all required columns", {
  for (col in expected_columns) {
    expect_true(col %in% names(penguins_raw),
                info = paste("Column", col, "should exist in raw data"))
  }
})

# Test 3: Data types are appropriate
test_that("Palmer penguins columns have appropriate data types", {
  expect_true(is.character(penguins_raw$species) || is.factor(penguins_raw$species),
              info = "Species should be character or factor")
  expect_true(is.numeric(penguins_raw$bill_length_mm),
              info = "Bill length should be numeric")
  expect_true(is.numeric(penguins_raw$bill_depth_mm),
              info = "Bill depth should be numeric")
  expect_true(is.numeric(penguins_raw$body_mass_g),
              info = "Body mass should be numeric")
})

# Test 4: Valid ranges for measurement variables
test_that("Palmer penguins measurements are within biologically plausible ranges", {
  # Bill length: expect 30-60mm based on penguin biology
  bill_lengths <- penguins_raw$bill_length_mm[!is.na(penguins_raw$bill_length_mm)]
  expect_true(all(bill_lengths >= 30 & bill_lengths <= 70),
              info = "Bill lengths should be between 30-70mm")

  # Bill depth: expect 10-25mm
  bill_depths <- penguins_raw$bill_depth_mm[!is.na(penguins_raw$bill_depth_mm)]
  expect_true(all(bill_depths >= 10 & bill_depths <= 25),
              info = "Bill depths should be between 10-25mm")

  # Body mass: expect 2000-7000g for penguins
  body_masses <- penguins_raw$body_mass_g[!is.na(penguins_raw$body_mass_g)]
  expect_true(all(body_masses >= 2000 & body_masses <= 7000),
              info = "Body masses should be between 2000-7000g")

  # Flipper length: expect 150-250mm
  flipper_lengths <- penguins_raw$flipper_length_mm[!is.na(penguins_raw$flipper_length_mm)]
  expect_true(all(flipper_lengths >= 150 & flipper_lengths <= 250),
              info = "Flipper lengths should be between 150-250mm")
})

# Test 5: Valid categorical values
test_that("Palmer penguins categorical variables have expected values", {
  # Species should be one of three known penguin species
  valid_species <- c("Adelie", "Chinstrap", "Gentoo")
  expect_true(all(penguins_raw$species %in% valid_species),
              info = "Species should be Adelie, Chinstrap, or Gentoo")

  # Islands should be research station locations
  valid_islands <- c("Biscoe", "Dream", "Torgersen")
  expect_true(all(penguins_raw$island %in% valid_islands),
              info = "Islands should be Biscoe, Dream, or Torgersen")

  # Sex should be male, female, or NA
  valid_sex <- c("male", "female", NA)
  expect_true(all(penguins_raw$sex %in% valid_sex),
              info = "Sex should be 'male', 'female', or missing")

  # Years should be within study period
  valid_years <- c(2007, 2008, 2009)
  expect_true(all(penguins_raw$year %in% valid_years),
              info = "Years should be 2007, 2008, or 2009")
})

# Test 6: Missing data patterns are expected
test_that("Missing data patterns match known data collection issues", {
  # Sex has known missing values (should be 5% or less)
  sex_missing_pct <- sum(is.na(penguins_raw$sex)) / nrow(penguins_raw) * 100
  expect_true(sex_missing_pct <= 15,
              info = "Sex missing percentage should be reasonable (<15%)")

  # Bill measurements should have minimal missing data
  bill_length_missing <- sum(is.na(penguins_raw$bill_length_mm))
  bill_depth_missing <- sum(is.na(penguins_raw$bill_depth_mm))
  expect_true(bill_length_missing <= 5,
              info = "Bill length should have minimal missing values")
  expect_true(bill_depth_missing <= 5,
              info = "Bill depth should have minimal missing values")
})

# Test 7: No duplicate observations
test_that("No duplicate penguin observations exist", {
  # Check for completely identical rows
  n_duplicates <- sum(duplicated(penguins_raw))
  expect_equal(n_duplicates, 0,
               info = "Should have no completely duplicate observations")

  # Check for suspicious near-duplicates in measurements
  if (nrow(penguins_raw) > 1) {
    for (i in 1:(nrow(penguins_raw)-1)) {
      for (j in (i+1):nrow(penguins_raw)) {
        if (!any(is.na(c(penguins_raw$bill_length_mm[i], penguins_raw$bill_depth_mm[i],
                          penguins_raw$bill_length_mm[j], penguins_raw$bill_depth_mm[j])))) {
          bill_diff <- abs(penguins_raw$bill_length_mm[i] - penguins_raw$bill_length_mm[j]) +
                      abs(penguins_raw$bill_depth_mm[i] - penguins_raw$bill_depth_mm[j])
          # No two penguins should have identical bill measurements (within 0.1mm)
          expect_true(bill_diff > 0.1 ||
                     penguins_raw$species[i] != penguins_raw$species[j],
                     info = "Suspicious identical measurements detected")
        }
      }
    }
  }
})

message("✅ All raw data validation tests passed!")

# =============================================================================
# DATA QUALITY ASSESSMENT
# =============================================================================

message("\n3. DATA QUALITY ASSESSMENT")

# Comprehensive data summary
skim_summary <- skim(penguins_raw)
print(skim_summary)

# Detailed quality assessment
quality_report <- list(
  total_rows = nrow(penguins_raw),
  total_cols = ncol(penguins_raw),
  missing_data = sapply(penguins_raw, function(x) sum(is.na(x))),
  missing_percent = round(sapply(penguins_raw, function(x) sum(is.na(x))/length(x) * 100), 2),
  duplicate_rows = sum(duplicated(penguins_raw)),
  unique_values = sapply(penguins_raw, function(x) length(unique(x))),
  species_counts = table(penguins_raw$species),
  island_counts = table(penguins_raw$island),
  sex_counts = table(penguins_raw$sex, useNA = "ifany")
)

# Print quality summary
message("Missing data summary:")
print(quality_report$missing_data[quality_report$missing_data > 0])

message("Species distribution:")
print(quality_report$species_counts)

# Save quality report
saveRDS(quality_report, file.path(here("analysis", "exploratory"), "quality_report.rds"))

# =============================================================================
# DATA CLEANING AND PREPARATION
# =============================================================================

message("\n4. DATA CLEANING AND PREPARATION")

# Create analysis-ready dataset
penguins_clean <- penguins_raw %>%
  # Remove rows with missing bill measurements (our focus variables)
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
  # Clean column names
  clean_names() %>%
  # Add derived variables for analysis
  mutate(
    # Bill ratio for shape analysis
    bill_ratio = bill_length_mm / bill_depth_mm,
    # Total bill size proxy
    bill_area_proxy = bill_length_mm * bill_depth_mm,
    # Body condition index
    body_condition = body_mass_g / flipper_length_mm,
    # Year as factor for temporal analysis
    year = as.factor(year),
    # Comprehensive ID for tracking
    penguin_id = paste(species, island, row_number(), sep = "_")
  ) %>%
  # Arrange by species for systematic analysis
  arrange(species, island)

message("Clean dataset: ", nrow(penguins_clean), " complete observations")
message("Derived variables added: bill_ratio, bill_area_proxy, body_condition")

# Save processed dataset
write_csv(penguins_clean, file.path(processed_data_dir, "penguins_clean.csv"))

# =============================================================================
# PROCESSED DATA UNIT TESTING (Validation After Transformation)
# =============================================================================

message("\n5. PROCESSED DATA UNIT TESTING")

# Validate the cleaned and transformed dataset
message("Running unit tests on processed Palmer penguins data...")

# Test 1: Data cleaning preserved essential observations
test_that("Data cleaning process preserved data integrity", {
  # Should retain most observations (at least 95% for this high-quality dataset)
  retention_rate <- nrow(penguins_clean) / nrow(penguins_raw)
  expect_true(retention_rate >= 0.95,
              info = paste("Data retention rate should be ≥95%, got", round(retention_rate * 100, 1), "%"))

  # Should have no missing values in key analysis variables
  expect_equal(sum(is.na(penguins_clean$bill_length_mm)), 0,
               info = "Processed data should have no missing bill lengths")
  expect_equal(sum(is.na(penguins_clean$bill_depth_mm)), 0,
               info = "Processed data should have no missing bill depths")
})

# Test 2: Derived variables are mathematically correct
test_that("Derived variables are calculated correctly", {
  # Test bill_ratio calculation
  expected_ratios <- penguins_clean$bill_length_mm / penguins_clean$bill_depth_mm
  expect_equal(penguins_clean$bill_ratio, expected_ratios,
               info = "Bill ratio should equal length/depth")

  # Test bill_area_proxy calculation
  expected_areas <- penguins_clean$bill_length_mm * penguins_clean$bill_depth_mm
  expect_equal(penguins_clean$bill_area_proxy, expected_areas,
               info = "Bill area proxy should equal length×depth")

  # Test body_condition calculation
  expected_condition <- penguins_clean$body_mass_g / penguins_clean$flipper_length_mm
  expect_equal(penguins_clean$body_condition, expected_condition,
               info = "Body condition should equal mass/flipper length")
})

# Test 3: Derived variables have biologically sensible ranges
test_that("Derived variables fall within expected biological ranges", {
  # Bill ratio should be between 1.5-4.0 (length usually 1.5-4x depth)
  expect_true(all(penguins_clean$bill_ratio >= 1.0 & penguins_clean$bill_ratio <= 5.0),
              info = "Bill ratios should be between 1.0-5.0")

  # Bill area proxy should be reasonable (200-1500 mm²)
  expect_true(all(penguins_clean$bill_area_proxy >= 200 & penguins_clean$bill_area_proxy <= 1500),
              info = "Bill area proxies should be between 200-1500 mm²")

  # Body condition should be reasonable (10-40 g/mm)
  expect_true(all(penguins_clean$body_condition >= 10 & penguins_clean$body_condition <= 40),
              info = "Body condition indices should be between 10-40 g/mm")
})

# Test 4: Species balance is maintained after cleaning
test_that("Species representation is maintained after data cleaning", {
  species_counts_raw <- table(penguins_raw$species)
  species_counts_clean <- table(penguins_clean$species)

  # Each species should retain at least 90% of observations
  for (species in names(species_counts_raw)) {
    retention <- species_counts_clean[species] / species_counts_raw[species]
    expect_true(retention >= 0.90,
                info = paste("Species", species, "should retain ≥90% of observations"))
  }

  # All three species should still be present
  expect_equal(length(species_counts_clean), 3,
               info = "All three penguin species should remain after cleaning")
})

# Test 5: Unique identifier integrity
test_that("Penguin IDs are unique and properly formatted", {
  # All penguin IDs should be unique
  expect_equal(length(unique(penguins_clean$penguin_id)), nrow(penguins_clean),
               info = "All penguin IDs should be unique")

  # IDs should follow expected format: species_island_number
  id_pattern <- "^(Adelie|Chinstrap|Gentoo)_(Biscoe|Dream|Torgersen)_[0-9]+$"
  expect_true(all(grepl(id_pattern, penguins_clean$penguin_id)),
              info = "Penguin IDs should follow format: species_island_number")
})

# Test 6: Data transformation didn't introduce artifacts
test_that("Data transformation preserved measurement relationships", {
  # Bill measurements should still be positively correlated
  bill_correlation <- cor(penguins_clean$bill_length_mm, penguins_clean$bill_depth_mm,
                         use = "complete.obs")
  expect_true(!is.na(bill_correlation),
              info = "Bill length and depth correlation should be calculable")

  # Body mass and flipper length should be positively correlated
  body_correlation <- cor(penguins_clean$body_mass_g, penguins_clean$flipper_length_mm,
                         use = "complete.obs")
  expect_true(body_correlation > 0,
              info = "Body mass and flipper length should be positively correlated")
})

# Test 7: Data export integrity (file system validation)
test_that("Processed data file was saved correctly", {
  saved_file_path <- file.path(processed_data_dir, "penguins_clean.csv")
  expect_true(file.exists(saved_file_path),
              info = "Processed data file should exist")

  # Read back the saved file and verify it matches
  penguins_reloaded <- read_csv(saved_file_path, show_col_types = FALSE)
  expect_equal(nrow(penguins_reloaded), nrow(penguins_clean),
               info = "Reloaded data should have same number of rows")
  expect_equal(ncol(penguins_reloaded), ncol(penguins_clean),
               info = "Reloaded data should have same number of columns")
})

message("✅ All processed data validation tests passed!")

# Create comprehensive data testing report
data_testing_report <- list(
  raw_data_tests = list(
    n_observations = nrow(penguins_raw),
    n_variables = ncol(penguins_raw),
    missing_data_check = "PASSED",
    range_validation = "PASSED",
    categorical_validation = "PASSED",
    duplicate_check = "PASSED"
  ),
  processed_data_tests = list(
    n_observations_clean = nrow(penguins_clean),
    retention_rate = round(nrow(penguins_clean) / nrow(penguins_raw), 3),
    derived_variables = c("bill_ratio", "bill_area_proxy", "body_condition", "penguin_id"),
    mathematical_validation = "PASSED",
    biological_validation = "PASSED",
    export_validation = "PASSED"
  ),
  testing_timestamp = Sys.time(),
  testing_status = "ALL TESTS PASSED - DATA READY FOR ANALYSIS"
)

# Save testing report for reproducibility documentation
saveRDS(data_testing_report, file.path(here("analysis", "exploratory"), "data_testing_report.rds"))

message("📊 Data testing report saved for reproducibility documentation")

# =============================================================================
# SYSTEMATIC UNIVARIATE ANALYSIS
# =============================================================================

message("\n6. SYSTEMATIC UNIVARIATE ANALYSIS")

# Bill length distribution analysis
bill_length_plot <- ggplot(penguins_clean, aes(x = bill_length_mm)) +
  geom_histogram(bins = 25, alpha = 0.7, fill = "steelblue", color = "white") +
  geom_density(aes(y = after_stat(count)), color = "red", linewidth = 1) +
  facet_wrap(~ species, scales = "free_y") +
  labs(
    title = "Bill Length Distribution by Species",
    subtitle = paste("n =", nrow(penguins_clean), "penguins with complete bill measurements"),
    x = "Bill Length (mm)",
    y = "Frequency",
    caption = "Source: Palmer Penguins dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    strip.text = element_text(face = "bold")
  )

ggsave(file.path(figures_dir, "bill_length_distribution.png"),
       bill_length_plot, width = 12, height = 6, dpi = 300)

# Bill depth distribution analysis
bill_depth_plot <- ggplot(penguins_clean, aes(x = bill_depth_mm)) +
  geom_histogram(bins = 25, alpha = 0.7, fill = "darkgreen", color = "white") +
  geom_density(aes(y = after_stat(count)), color = "orange", linewidth = 1) +
  facet_wrap(~ species, scales = "free_y") +
  labs(
    title = "Bill Depth Distribution by Species",
    subtitle = "Different species show distinct bill depth characteristics",
    x = "Bill Depth (mm)",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave(file.path(figures_dir, "bill_depth_distribution.png"),
       bill_depth_plot, width = 12, height = 6, dpi = 300)

# Summary statistics by species
species_summary <- penguins_clean %>%
  group_by(species) %>%
  summarise(
    n = n(),
    bill_length_mean = round(mean(bill_length_mm), 2),
    bill_length_sd = round(sd(bill_length_mm), 2),
    bill_depth_mean = round(mean(bill_depth_mm), 2),
    bill_depth_sd = round(sd(bill_depth_mm), 2),
    bill_ratio_mean = round(mean(bill_ratio), 3),
    body_mass_mean = round(mean(body_mass_g, na.rm = TRUE), 0),
    .groups = "drop"
  )

write_csv(species_summary, file.path(here("outputs", "tables"), "species_summary.csv"))
message("Species summary statistics saved")

# =============================================================================
# BIVARIATE RELATIONSHIP ANALYSIS
# =============================================================================

message("\n5. BIVARIATE RELATIONSHIP ANALYSIS")

# Primary analysis: Bill depth vs length by species
bill_scatter <- ggplot(penguins_clean, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(aes(color = species, shape = species), size = 2.5, alpha = 0.8) +
  geom_smooth(aes(color = species), method = "lm", se = TRUE, alpha = 0.3) +
  scale_color_manual(values = c("Adelie" = "#FF6B35", "Chinstrap" = "#004E89", "Gentoo" = "#00A896")) +
  labs(
    title = "Palmer Penguins: Bill Depth vs Length by Species",
    subtitle = "Clear species-specific clustering with distinct linear relationships",
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "Species",
    shape = "Species",
    caption = "Each species shows unique bill dimension characteristics"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "bottom",
    panel.border = element_rect(color = "black", fill = NA)
  )

ggsave(file.path(figures_dir, "bill_dimensions_scatter.png"),
       bill_scatter, width = 10, height = 8, dpi = 300)

# Interactive version for exploration
bill_interactive <- ggplotly(bill_scatter, tooltip = c("x", "y", "colour"))
htmlwidgets::saveWidget(bill_interactive,
                       file.path(figures_dir, "bill_dimensions_interactive.html"))

# Correlation analysis by species
correlation_analysis <- penguins_clean %>%
  group_by(species) %>%
  summarise(
    bill_correlation = cor(bill_length_mm, bill_depth_mm),
    n_observations = n(),
    .groups = "drop"
  ) %>%
  mutate(
    correlation_strength = case_when(
      abs(bill_correlation) >= 0.7 ~ "Strong",
      abs(bill_correlation) >= 0.4 ~ "Moderate",
      TRUE ~ "Weak"
    )
  )

write_csv(correlation_analysis, file.path(here("outputs", "tables"), "bill_correlations.csv"))

# =============================================================================
# ADVANCED EXPLORATORY VISUALIZATIONS
# =============================================================================

message("\n6. ADVANCED EXPLORATORY VISUALIZATIONS")

# Multi-dimensional analysis
multi_plot <- penguins_clean %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%
  tidyr::pivot_longer(cols = -species, names_to = "measurement", values_to = "value") %>%
  ggplot(aes(x = species, y = value, fill = species)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.5) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 0.5) +
  facet_wrap(~ measurement, scales = "free_y",
             labeller = labeller(measurement = c(
               "bill_length_mm" = "Bill Length (mm)",
               "bill_depth_mm" = "Bill Depth (mm)",
               "flipper_length_mm" = "Flipper Length (mm)",
               "body_mass_g" = "Body Mass (g)"
             ))) +
  scale_fill_manual(values = c("Adelie" = "#FF6B35", "Chinstrap" = "#004E89", "Gentoo" = "#00A896")) +
  labs(
    title = "Palmer Penguins: Comprehensive Morphometric Analysis",
    subtitle = "Species differences across all physical measurements",
    x = "Species",
    y = "Measurement Value",
    fill = "Species"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

ggsave(file.path(figures_dir, "comprehensive_morphometrics.png"),
       multi_plot, width = 12, height = 8, dpi = 300)

# =============================================================================
# EXPLORATORY INSIGHTS AND RECOMMENDATIONS
# =============================================================================

# Document key findings for modeling phase
exploratory_insights <- list(
  timestamp = Sys.time(),
  key_findings = list(
    "Species form distinct clusters in bill dimension space",
    "Adelie penguins: shorter, deeper bills (different feeding ecology)",
    "Gentoo penguins: longest bills, intermediate depth",
    "Chinstrap penguins: long bills, shallow depth",
    "Strong linear relationships within species",
    "Bill ratio could be effective species discriminator"
  ),
  modeling_recommendations = list(
    "Bill dimensions highly predictive of species",
    "Consider interaction terms between bill length and depth",
    "Body mass and flipper length provide additional discrimination",
    "Potential for high-accuracy classification models",
    "Island may provide additional geographic signal"
  ),
  data_quality_notes = list(
    paste("Complete cases:", nrow(penguins_clean), "of", nrow(penguins_raw), "original observations"),
    "Missing data primarily in sex variable (not critical for bill analysis)",
    "No obvious data entry errors or extreme outliers detected",
    "Species sample sizes well-balanced for statistical analysis"
  )
)

saveRDS(exploratory_insights, file.path(here("analysis", "exploratory"), "eda_insights.rds"))

# Complete session documentation
session_log$end_time <- Sys.time()
session_log$duration <- difftime(session_log$end_time, session_log$start_time, units = "mins")
session_log$files_created <- list.files(figures_dir, pattern = "\\.(png|html)$")
session_log$session_info <- sessionInfo()

saveRDS(session_log, file.path(here("analysis", "exploratory"), "eda_session_log.rds"))

message("\n=== EXPLORATORY ANALYSIS COMPLETED ===")
message("Duration: ", round(as.numeric(session_log$duration), 2), " minutes")
message("Figures created: ", length(session_log$files_created))
message("Key finding: Species show distinct bill dimension characteristics")
message("Recommendation: Proceed to statistical modeling with species classification")

# Print session info for reproducibility
print(sessionInfo())
```

### Step 3: Statistical Modeling Pipeline

Edit `scripts/02_statistical_modeling.R`:

```r
# Statistical Modeling - Palmer Penguins Species Classification
# Project: penguin-analysis
# Focus: Develop predictive models for species classification using bill measurements

# Load required packages (tidymodels ecosystem pre-configured)
library(here)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(readr)
library(broom)

# Set reproducible seed for all random operations
set.seed(123)

# Session documentation
session_log <- list(
  script = "02_statistical_modeling.R",
  start_time = Sys.time(),
  r_version = R.version.string,
  tidymodels_version = packageVersion("tidymodels"),
  random_seed = 123,
  modeling_objective = "Species classification using bill measurements"
)

# Set up paths
processed_data_dir <- here("data", "processed")
modeling_dir <- here("analysis", "modeling")
figures_dir <- here("outputs", "figures")
tables_dir <- here("outputs", "tables")

# Create modeling directory
if (!dir.exists(modeling_dir)) dir.create(modeling_dir, recursive = TRUE)

message("Starting tidymodels classification pipeline...")
message("Objective: Predict penguin species from bill measurements")

# =============================================================================
# DATA LOADING AND PREPARATION
# =============================================================================

message("\n1. DATA LOADING AND PREPARATION")

# Load cleaned data from exploratory phase
penguins_clean <- read_csv(file.path(processed_data_dir, "penguins_clean.csv"))

# Prepare modeling dataset
penguins_model <- penguins_clean %>%
  # Focus on complete cases for bill measurements and species
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm), !is.na(species)) %>%
  # Convert species to factor for classification
  mutate(species = as.factor(species)) %>%
  # Select modeling variables
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm,
         body_mass_g, bill_ratio, bill_area_proxy, island, sex)

message("Modeling dataset: ", nrow(penguins_model), " observations")
message("Target variable: species (", nlevels(penguins_model$species), " levels)")
message("Predictor variables: ", ncol(penguins_model) - 1)

# =============================================================================
# DATA SPLITTING STRATEGY
# =============================================================================

message("\n2. DATA SPLITTING STRATEGY")

# Stratified split to maintain species balance
set.seed(123)
penguin_split <- initial_split(penguins_model, prop = 0.75, strata = species)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)

# Cross-validation folds for model tuning
penguin_folds <- vfold_cv(penguin_train, v = 5, strata = species)

# Document split characteristics
split_summary <- list(
  train_n = nrow(penguin_train),
  test_n = nrow(penguin_test),
  train_species = table(penguin_train$species),
  test_species = table(penguin_test$species),
  cv_folds = 5
)

message("Training set: ", split_summary$train_n, " observations")
message("Test set: ", split_summary$test_n, " observations")
message("Training species distribution:")
print(split_summary$train_species)

# =============================================================================
# FEATURE ENGINEERING RECIPE
# =============================================================================

message("\n3. FEATURE ENGINEERING RECIPE")

# Comprehensive preprocessing recipe
penguin_recipe <- recipe(species ~ ., data = penguin_train) %>%
  # Handle missing values
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # Normalize numeric predictors for algorithms that need it
  step_normalize(all_numeric_predictors()) %>%
  # Create polynomial features for bill measurements (capture non-linearity)
  step_poly(bill_length_mm, bill_depth_mm, degree = 2, keep_original = TRUE) %>%
  # Create interaction terms
  step_interact(terms = ~ bill_length_mm:bill_depth_mm) %>%
  # Convert categorical variables to dummy variables
  step_dummy(all_nominal_predictors()) %>%
  # Remove zero variance predictors
  step_zv(all_predictors()) %>%
  # Remove highly correlated predictors
  step_corr(all_numeric_predictors(), threshold = 0.9)

# Preview recipe
penguin_recipe
message("Recipe created with comprehensive feature engineering")

# =============================================================================
# MODEL SPECIFICATIONS
# =============================================================================

message("\n4. MODEL SPECIFICATIONS")

# Logistic Regression (baseline interpretable model)
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Random Forest (ensemble method)
rf_spec <- rand_forest(
  trees = tune(),
  min_n = tune(),
  mtry = tune()
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Support Vector Machine (kernel method)
svm_spec <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune()
) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

# K-Nearest Neighbors (instance-based)
knn_spec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

message("Model specifications created: Logistic, Random Forest, SVM, KNN")

# =============================================================================
# WORKFLOW CREATION
# =============================================================================

message("\n5. WORKFLOW CREATION")

# Create workflows combining recipes and models
logistic_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(logistic_spec)

rf_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(rf_spec)

svm_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(svm_spec)

knn_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(knn_spec)

# Create workflow set for easy comparison
penguin_wfset <- workflow_set(
  preproc = list(engineered = penguin_recipe),
  models = list(
    logistic = logistic_spec,
    random_forest = rf_spec,
    svm_rbf = svm_spec,
    knn = knn_spec
  )
)

# =============================================================================
# HYPERPARAMETER TUNING
# =============================================================================

message("\n6. HYPERPARAMETER TUNING")

# Define metrics for evaluation
classification_metrics <- metric_set(accuracy, roc_auc, sensitivity, specificity)

# Tune Random Forest (most important tuning for this dataset)
rf_grid <- grid_regular(
  trees(range = c(50, 500)),
  min_n(range = c(2, 20)),
  mtry(range = c(2, 6)),
  levels = 3
)

message("Tuning Random Forest model...")
rf_tuned <- tune_grid(
  rf_wf,
  resamples = penguin_folds,
  grid = rf_grid,
  metrics = classification_metrics,
  control = control_grid(save_pred = TRUE, verbose = FALSE)
)

# Tune SVM
svm_grid <- grid_regular(
  cost(range = c(-3, 2)),
  rbf_sigma(range = c(-4, -1)),
  levels = 4
)

message("Tuning SVM model...")
svm_tuned <- tune_grid(
  svm_wf,
  resamples = penguin_folds,
  grid = svm_grid,
  metrics = classification_metrics,
  control = control_grid(save_pred = TRUE, verbose = FALSE)
)

# Tune KNN
knn_grid <- grid_regular(
  neighbors(range = c(3, 15)),
  weight_func(values = c("rectangular", "triangular", "gaussian")),
  dist_power(range = c(1, 2)),
  levels = c(5, 3, 2)
)

message("Tuning KNN model...")
knn_tuned <- tune_grid(
  knn_wf,
  resamples = penguin_folds,
  grid = knn_grid,
  metrics = classification_metrics,
  control = control_grid(save_pred = TRUE, verbose = FALSE)
)

# =============================================================================
# MODEL EVALUATION AND SELECTION
# =============================================================================

message("\n7. MODEL EVALUATION AND SELECTION")

# Extract best parameters for each model
rf_best <- select_best(rf_tuned, metric = "accuracy")
svm_best <- select_best(svm_tuned, metric = "accuracy")
knn_best <- select_best(knn_tuned, metric = "accuracy")

# Finalize workflows with best parameters
rf_final_wf <- finalize_workflow(rf_wf, rf_best)
svm_final_wf <- finalize_workflow(svm_wf, svm_best)
knn_final_wf <- finalize_workflow(knn_wf, knn_best)

# Fit logistic regression (no tuning needed)
logistic_final <- fit(logistic_wf, data = penguin_train)

# Fit tuned models on full training data
rf_final <- fit(rf_final_wf, data = penguin_train)
svm_final <- fit(svm_final_wf, data = penguin_train)
knn_final <- fit(knn_final_wf, data = penguin_train)

# Evaluate all models on test set
models <- list(
  "Logistic Regression" = logistic_final,
  "Random Forest" = rf_final,
  "SVM RBF" = svm_final,
  "K-Nearest Neighbors" = knn_final
)

# Comprehensive test set evaluation
test_results <- map_dfr(models, function(model) {
  predictions <- predict(model, new_data = penguin_test, type = "prob") %>%
    bind_cols(predict(model, new_data = penguin_test, type = "class")) %>%
    bind_cols(penguin_test %>% select(species))

  # Multi-class metrics
  metrics_df <- predictions %>%
    classification_metrics(truth = species, estimate = .pred_class,
                         .pred_Adelie, .pred_Chinstrap, .pred_Gentoo)

  return(metrics_df)
}, .id = "model")

# Create model comparison table
model_comparison <- test_results %>%
  select(model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  arrange(desc(accuracy))

write_csv(model_comparison, file.path(tables_dir, "model_comparison.csv"))

# Display best model
best_model_name <- model_comparison$model[1]
best_accuracy <- round(model_comparison$accuracy[1], 4)
message("Best model: ", best_model_name, " (Accuracy: ", best_accuracy, ")")

# =============================================================================
# DETAILED MODEL DIAGNOSTICS
# =============================================================================

message("\n8. DETAILED MODEL DIAGNOSTICS")

# Focus on best model (likely Random Forest)
best_model <- rf_final

# Generate detailed predictions
test_predictions <- predict(best_model, new_data = penguin_test, type = "prob") %>%
  bind_cols(predict(best_model, new_data = penguin_test, type = "class")) %>%
  bind_cols(penguin_test %>% select(species, bill_length_mm, bill_depth_mm)) %>%
  mutate(correct_prediction = species == .pred_class)

# Confusion matrix
conf_matrix <- test_predictions %>%
  conf_mat(truth = species, estimate = .pred_class)

print(conf_matrix)

# Confusion matrix visualization
conf_matrix_plot <- autoplot(conf_matrix, type = "heatmap") +
  labs(title = "Confusion Matrix - Species Classification",
       subtitle = paste("Model:", best_model_name, "- Test Set Accuracy:", best_accuracy)) +
  theme_minimal()

ggsave(file.path(figures_dir, "confusion_matrix.png"),
       conf_matrix_plot, width = 8, height = 6, dpi = 300)

# Feature importance (for Random Forest)
if (best_model_name == "Random Forest") {
  importance_data <- extract_fit_engine(best_model)$variable.importance %>%
    tibble::enframe(name = "variable", value = "importance") %>%
    arrange(desc(importance)) %>%
    slice_head(n = 10)

  importance_plot <- importance_data %>%
    ggplot(aes(x = reorder(variable, importance), y = importance)) +
    geom_col(fill = "steelblue", alpha = 0.8) +
    coord_flip() +
    labs(
      title = "Feature Importance - Random Forest Model",
      subtitle = "Top 10 most important variables for species classification",
      x = "Variables",
      y = "Importance Score"
    ) +
    theme_minimal()

  ggsave(file.path(figures_dir, "feature_importance.png"),
         importance_plot, width = 10, height = 6, dpi = 300)

  write_csv(importance_data, file.path(tables_dir, "feature_importance.csv"))
}

# Decision boundary visualization
decision_boundary_plot <- penguin_test %>%
  mutate(prediction = predict(best_model, new_data = penguin_test)$.pred_class) %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(aes(color = species, shape = prediction), size = 3, alpha = 0.8) +
  scale_color_manual(values = c("Adelie" = "#FF6B35", "Chinstrap" = "#004E89", "Gentoo" = "#00A896")) +
  scale_shape_manual(values = c("Adelie" = 16, "Chinstrap" = 17, "Gentoo" = 15)) +
  labs(
    title = "Species Classification Results on Test Set",
    subtitle = "Color = True Species, Shape = Predicted Species",
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "Actual Species",
    shape = "Predicted Species"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggsave(file.path(figures_dir, "classification_results.png"),
       decision_boundary_plot, width = 10, height = 8, dpi = 300)

# =============================================================================
# MODEL PERSISTENCE AND DOCUMENTATION
# =============================================================================

# Save final model for deployment
saveRDS(best_model, file.path(modeling_dir, "final_classification_model.rds"))

# Save model metadata
model_metadata <- list(
  model_name = best_model_name,
  test_accuracy = best_accuracy,
  training_data_size = nrow(penguin_train),
  test_data_size = nrow(penguin_test),
  features_used = names(penguin_model)[-1],
  best_parameters = if(best_model_name == "Random Forest") rf_best else "No tuning",
  cv_performance = if(best_model_name == "Random Forest") collect_metrics(rf_tuned) else NULL,
  confusion_matrix = conf_matrix,
  timestamp = Sys.time()
)

saveRDS(model_metadata, file.path(modeling_dir, "model_metadata.rds"))

# Complete session documentation
session_log$end_time <- Sys.time()
session_log$duration <- difftime(session_log$end_time, session_log$start_time, units = "mins")
session_log$best_model <- best_model_name
session_log$final_accuracy <- best_accuracy
session_log$session_info <- sessionInfo()

saveRDS(session_log, file.path(modeling_dir, "modeling_session_log.rds"))

message("\n=== STATISTICAL MODELING COMPLETED ===")
message("Duration: ", round(as.numeric(session_log$duration), 2), " minutes")
message("Best model: ", best_model_name)
message("Test accuracy: ", best_accuracy)
message("Model saved to: analysis/modeling/final_classification_model.rds")

# Print session info for reproducibility
print(sessionInfo())
```

### Step 4: Create Analysis Functions

Edit `R/analysis_functions.R`:

```r
#' Load and validate Palmer Penguins data
#'
#' Loads cleaned penguin data with validation checks for analysis workflows.
#'
#' @param file_path Character string path to the cleaned data file
#' @param validate_completeness Logical, whether to require complete cases
#' @return Data frame with validated penguin data
#' @export
#' @examples
#' penguins <- load_penguin_data("data/processed/penguins_clean.csv")
load_penguin_data <- function(file_path, validate_completeness = TRUE) {
  if (!file.exists(file_path)) {
    stop("Data file not found: ", file_path)
  }

  data <- readr::read_csv(file_path, show_col_types = FALSE)

  # Validate required columns
  required_cols <- c("species", "bill_length_mm", "bill_depth_mm")
  missing_cols <- setdiff(required_cols, names(data))
  if (length(missing_cols) > 0) {
    stop("Required columns missing: ", paste(missing_cols, collapse = ", "))
  }

  # Validate data completeness if requested
  if (validate_completeness) {
    complete_cases <- sum(complete.cases(data[required_cols]))
    if (complete_cases < nrow(data)) {
      warning("Data contains ", nrow(data) - complete_cases, " incomplete cases")
    }
  }

  message("Penguin data loaded: ", nrow(data), " observations, ", ncol(data), " variables")
  return(data)
}

#' Create bill dimension scatter plot
#'
#' Generates a professional scatter plot of penguin bill dimensions with species coloring.
#'
#' @param data Data frame containing penguin measurements
#' @param add_regression Logical, whether to add regression lines
#' @param interactive Logical, whether to create interactive plotly version
#' @return ggplot2 object or plotly object if interactive = TRUE
#' @export
#' @examples
#' plot <- create_bill_scatter(penguins_clean, add_regression = TRUE)
create_bill_scatter <- function(data, add_regression = FALSE, interactive = FALSE) {
  # Validate input data
  required_cols <- c("species", "bill_length_mm", "bill_depth_mm")
  if (!all(required_cols %in% names(data))) {
    stop("Data must contain: ", paste(required_cols, collapse = ", "))
  }

  # Create base plot
  p <- ggplot2::ggplot(data, ggplot2::aes(x = bill_length_mm, y = bill_depth_mm)) +
    ggplot2::geom_point(ggplot2::aes(color = species, shape = species),
                       size = 2.5, alpha = 0.8) +
    ggplot2::scale_color_manual(
      values = c("Adelie" = "#FF6B35", "Chinstrap" = "#004E89", "Gentoo" = "#00A896")
    ) +
    ggplot2::labs(
      title = "Palmer Penguins: Bill Depth vs Length",
      subtitle = "Species-specific clustering in bill morphology",
      x = "Bill Length (mm)",
      y = "Bill Depth (mm)",
      color = "Species",
      shape = "Species"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(
      plot.title = ggplot2::element_text(size = 14, face = "bold"),
      legend.position = "bottom"
    )

  # Add regression lines if requested
  if (add_regression) {
    p <- p + ggplot2::geom_smooth(ggplot2::aes(color = species),
                                 method = "lm", se = TRUE, alpha = 0.3)
  }

  # Convert to interactive if requested
  if (interactive) {
    if (!requireNamespace("plotly", quietly = TRUE)) {
      warning("plotly package not available, returning static plot")
      return(p)
    }
    p <- plotly::ggplotly(p, tooltip = c("x", "y", "colour"))
  }

  return(p)
}

#' Perform species classification analysis
#'
#' Trains and evaluates multiple classification models for penguin species prediction.
#'
#' @param data Data frame with penguin measurements
#' @param test_prop Proportion of data to use for testing
#' @param cv_folds Number of cross-validation folds
#' @return List containing trained models and evaluation metrics
#' @export
#' @examples
#' results <- classify_penguin_species(penguins_clean, test_prop = 0.3)
classify_penguin_species <- function(data, test_prop = 0.25, cv_folds = 5) {
  if (!requireNamespace("tidymodels", quietly = TRUE)) {
    stop("tidymodels package required for classification analysis")
  }

  # Prepare data
  model_data <- data %>%
    dplyr::filter(!is.na(species), !is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    dplyr::mutate(species = as.factor(species))

  # Data splitting
  set.seed(123)
  data_split <- rsample::initial_split(model_data, prop = 1 - test_prop, strata = species)
  train_data <- rsample::training(data_split)
  test_data <- rsample::testing(data_split)

  # Create recipe
  recipe <- recipes::recipe(species ~ bill_length_mm + bill_depth_mm +
                           flipper_length_mm + body_mass_g, data = train_data) %>%
    recipes::step_impute_median(recipes::all_numeric_predictors()) %>%
    recipes::step_normalize(recipes::all_numeric_predictors())

  # Random Forest specification
  rf_spec <- parsnip::rand_forest(trees = 500) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("classification")

  # Create workflow
  rf_workflow <- workflows::workflow() %>%
    workflows::add_recipe(recipe) %>%
    workflows::add_model(rf_spec)

  # Fit model
  rf_fit <- parsnip::fit(rf_workflow, data = train_data)

  # Evaluate on test set
  test_predictions <- predict(rf_fit, new_data = test_data) %>%
    dplyr::bind_cols(test_data %>% dplyr::select(species))

  # Calculate metrics
  accuracy <- yardstick::accuracy(test_predictions, truth = species, estimate = .pred_class)$.estimate

  # Return results
  list(
    model = rf_fit,
    test_accuracy = accuracy,
    test_predictions = test_predictions,
    train_size = nrow(train_data),
    test_size = nrow(test_data)
  )
}

#' Generate comprehensive data quality report
#'
#' Creates detailed data quality assessment for penguin datasets.
#'
#' @param data Data frame to assess
#' @param save_report Logical, whether to save report
#' @return List containing quality metrics
#' @export
#' @examples
#' quality <- assess_data_quality(penguins_data)
assess_data_quality <- function(data, save_report = FALSE) {
  # Basic dimensions
  quality_metrics <- list(
    timestamp = Sys.time(),
    dimensions = list(rows = nrow(data), cols = ncol(data)),
    column_names = names(data)
  )

  # Missing data analysis
  quality_metrics$missing_data <- sapply(data, function(x) sum(is.na(x)))
  quality_metrics$missing_percent <- round(quality_metrics$missing_data / nrow(data) * 100, 2)

  # Data types
  quality_metrics$data_types <- sapply(data, function(x) class(x)[1])

  # Unique values
  quality_metrics$unique_values <- sapply(data, function(x) length(unique(x[!is.na(x)])))

  # Duplicate rows
  quality_metrics$duplicate_rows <- sum(duplicated(data))

  # Numeric variable summaries
  numeric_vars <- names(dplyr::select_if(data, is.numeric))
  if (length(numeric_vars) > 0) {
    numeric_summary <- data %>%
      dplyr::select(dplyr::all_of(numeric_vars)) %>%
      dplyr::summarise_all(list(
        min = ~min(., na.rm = TRUE),
        q25 = ~quantile(., 0.25, na.rm = TRUE),
        median = ~median(., na.rm = TRUE),
        mean = ~round(mean(., na.rm = TRUE), 3),
        q75 = ~quantile(., 0.75, na.rm = TRUE),
        max = ~max(., na.rm = TRUE),
        sd = ~round(sd(., na.rm = TRUE), 3)
      ))
    quality_metrics$numeric_summary <- numeric_summary
  }

  # Categorical variable summaries
  categorical_vars <- names(dplyr::select_if(data, function(x) is.factor(x) | is.character(x)))
  if (length(categorical_vars) > 0) {
    categorical_summary <- lapply(categorical_vars, function(var) {
      table(data[[var]], useNA = "ifany")
    })
    names(categorical_summary) <- categorical_vars
    quality_metrics$categorical_summary <- categorical_summary
  }

  # Save report if requested
  if (save_report) {
    output_dir <- here::here("analysis", "exploratory")
    if (!dir.exists(output_dir)) {
      dir.create(output_dir, recursive = TRUE)
    }
    saveRDS(quality_metrics, file.path(output_dir, "data_quality_report.rds"))
    message("Quality report saved to: analysis/exploratory/data_quality_report.rds")
  }

  return(quality_metrics)
}
```

### Step 5: Create Comprehensive Tests

Edit `tests/testthat/test-analysis_functions.R`:

```r
# Tests for Palmer Penguins Analysis Functions

test_that("load_penguin_data works with valid data", {
  # Create temporary test data
  temp_dir <- tempdir()
  test_file <- file.path(temp_dir, "test_penguins.csv")

  # Create minimal test dataset
  test_data <- data.frame(
    species = c("Adelie", "Chinstrap", "Gentoo"),
    bill_length_mm = c(35.5, 48.2, 47.3),
    bill_depth_mm = c(18.2, 17.8, 14.2),
    flipper_length_mm = c(190, 195, 210),
    body_mass_g = c(3400, 3700, 4800)
  )

  readr::write_csv(test_data, test_file)

  # Test loading
  loaded_data <- load_penguin_data(test_file)

  expect_s3_class(loaded_data, "data.frame")
  expect_equal(nrow(loaded_data), 3)
  expect_true(all(c("species", "bill_length_mm", "bill_depth_mm") %in% names(loaded_data)))

  # Cleanup
  unlink(test_file)
})

test_that("load_penguin_data fails with missing file", {
  expect_error(load_penguin_data("nonexistent_file.csv"), "Data file not found")
})

test_that("load_penguin_data fails with missing required columns", {
  # Create temporary test data without required columns
  temp_dir <- tempdir()
  test_file <- file.path(temp_dir, "test_incomplete.csv")

  incomplete_data <- data.frame(
    name = c("A", "B", "C"),
    value = c(1, 2, 3)
  )

  readr::write_csv(incomplete_data, test_file)

  expect_error(load_penguin_data(test_file), "Required columns missing")

  # Cleanup
  unlink(test_file)
})

test_that("create_bill_scatter produces valid ggplot", {
  # Create test data
  test_data <- data.frame(
    species = factor(c("Adelie", "Chinstrap", "Gentoo", "Adelie", "Chinstrap")),
    bill_length_mm = c(35.5, 48.2, 47.3, 36.0, 49.0),
    bill_depth_mm = c(18.2, 17.8, 14.2, 18.5, 17.5)
  )

  # Test basic plot creation
  p <- create_bill_scatter(test_data)

  expect_s3_class(p, "ggplot")
  expect_equal(length(p$layers), 1)  # One geom_point layer

  # Test with regression lines
  p_reg <- create_bill_scatter(test_data, add_regression = TRUE)
  expect_s3_class(p_reg, "ggplot")
  expect_equal(length(p_reg$layers), 2)  # geom_point + geom_smooth
})

test_that("create_bill_scatter fails with missing columns", {
  incomplete_data <- data.frame(
    species = c("A", "B"),
    other_var = c(1, 2)
  )

  expect_error(create_bill_scatter(incomplete_data), "Data must contain")
})

test_that("classify_penguin_species returns valid results", {
  skip_if_not_installed("tidymodels")

  # Create sufficient test data for modeling
  set.seed(42)
  test_data <- data.frame(
    species = factor(rep(c("Adelie", "Chinstrap", "Gentoo"), each = 20)),
    bill_length_mm = c(rnorm(20, 38, 2), rnorm(20, 48, 2), rnorm(20, 47, 2)),
    bill_depth_mm = c(rnorm(20, 18, 1), rnorm(20, 17, 1), rnorm(20, 14, 1)),
    flipper_length_mm = c(rnorm(20, 190, 5), rnorm(20, 195, 5), rnorm(20, 215, 5)),
    body_mass_g = c(rnorm(20, 3700, 200), rnorm(20, 3700, 200), rnorm(20, 5000, 300))
  )

  # Test classification
  results <- classify_penguin_species(test_data, test_prop = 0.3, cv_folds = 3)

  expect_type(results, "list")
  expect_true("model" %in% names(results))
  expect_true("test_accuracy" %in% names(results))
  expect_true(results$test_accuracy >= 0 && results$test_accuracy <= 1)
  expect_true(results$train_size > 0)
  expect_true(results$test_size > 0)
})

test_that("assess_data_quality produces comprehensive report", {
  # Create test data with various quality issues
  test_data <- data.frame(
    id = 1:10,
    species = factor(c(rep("Adelie", 4), rep("Chinstrap", 3), rep("Gentoo", 3))),
    bill_length_mm = c(35, 36, NA, 38, 48, 49, NA, 46, 47, 45),
    bill_depth_mm = c(18, 19, 17, NA, 17, 18, 16, 14, 15, 14),
    sex = c("male", "female", "male", NA, "female", "male", "female", NA, "male", "female"),
    duplicate_row = rep(1, 10)  # Zero variance column
  )

  # Add a duplicate row
  test_data <- rbind(test_data, test_data[1, ])

  quality_report <- assess_data_quality(test_data)

  # Test report structure
  expect_type(quality_report, "list")
  expect_true("dimensions" %in% names(quality_report))
  expect_true("missing_data" %in% names(quality_report))
  expect_true("missing_percent" %in% names(quality_report))
  expect_true("data_types" %in% names(quality_report))
  expect_true("duplicate_rows" %in% names(quality_report))

  # Test specific values
  expect_equal(quality_report$dimensions$rows, 11)  # 10 + 1 duplicate
  expect_equal(quality_report$dimensions$cols, 6)
  expect_equal(quality_report$duplicate_rows, 1)

  # Test missing data detection
  expect_true(quality_report$missing_data["bill_length_mm"] > 0)
  expect_true(quality_report$missing_percent["bill_length_mm"] > 0)

  # Test numeric summary exists
  expect_true("numeric_summary" %in% names(quality_report))

  # Test categorical summary exists
  expect_true("categorical_summary" %in% names(quality_report))
  expect_true("species" %in% names(quality_report$categorical_summary))
})

test_that("assess_data_quality handles edge cases", {
  # Empty data frame
  empty_data <- data.frame()
  quality_empty <- assess_data_quality(empty_data)
  expect_equal(quality_empty$dimensions$rows, 0)
  expect_equal(quality_empty$dimensions$cols, 0)

  # Data frame with only numeric variables
  numeric_only <- data.frame(
    x = 1:5,
    y = 6:10,
    z = c(1.1, 2.2, 3.3, 4.4, 5.5)
  )
  quality_numeric <- assess_data_quality(numeric_only)
  expect_true("numeric_summary" %in% names(quality_numeric))
  expect_false("categorical_summary" %in% names(quality_numeric))

  # Data frame with only categorical variables
  categorical_only <- data.frame(
    group = factor(c("A", "B", "A", "B", "C")),
    category = c("X", "Y", "X", "Z", "Y")
  )
  quality_categorical <- assess_data_quality(categorical_only)
  expect_false("numeric_summary" %in% names(quality_categorical))
  expect_true("categorical_summary" %in% names(quality_categorical))
})

# Integration test
test_that("full analysis workflow integration works", {
  skip_if_not_installed("tidymodels")

  # Create comprehensive test dataset
  set.seed(42)
  n_per_species <- 30
  test_data <- data.frame(
    species = factor(rep(c("Adelie", "Chinstrap", "Gentoo"), each = n_per_species)),
    bill_length_mm = c(
      rnorm(n_per_species, 38, 2),    # Adelie
      rnorm(n_per_species, 48, 2),    # Chinstrap
      rnorm(n_per_species, 47, 2)     # Gentoo
    ),
    bill_depth_mm = c(
      rnorm(n_per_species, 18, 1),    # Adelie (deeper bills)
      rnorm(n_per_species, 17, 1),    # Chinstrap
      rnorm(n_per_species, 14, 1)     # Gentoo (shallower bills)
    ),
    flipper_length_mm = c(
      rnorm(n_per_species, 190, 5),   # Adelie
      rnorm(n_per_species, 195, 5),   # Chinstrap
      rnorm(n_per_species, 215, 5)    # Gentoo (longer flippers)
    ),
    body_mass_g = c(
      rnorm(n_per_species, 3700, 200), # Adelie
      rnorm(n_per_species, 3700, 200), # Chinstrap
      rnorm(n_per_species, 5000, 300)  # Gentoo (heavier)
    )
  )

  # Save to temporary file
  temp_dir <- tempdir()
  test_file <- file.path(temp_dir, "integration_test_penguins.csv")
  readr::write_csv(test_data, test_file)

  # Test complete workflow
  expect_no_error({
    # Load data
    penguins <- load_penguin_data(test_file)

    # Quality assessment
    quality <- assess_data_quality(penguins)

    # Visualization
    plot <- create_bill_scatter(penguins, add_regression = TRUE)

    # Classification
    classification_results <- classify_penguin_species(penguins, test_prop = 0.3)
  })

  # Verify results
  expect_true(classification_results$test_accuracy > 0.7)  # Should be high accuracy
  expect_s3_class(plot, "ggplot")
  expect_true(quality$dimensions$rows == nrow(test_data))

  # Cleanup
  unlink(test_file)
})
```

### Step 6: Execute the Complete Analysis

```bash
# Inside the container, run the complete analysis pipeline
R --vanilla < scripts/01_exploratory_analysis.R
R --vanilla < scripts/02_statistical_modeling.R

# Test your functions
R
library(devtools)
load_all()
test()
quit()

# Check outputs
ls outputs/figures/
ls outputs/tables/
ls analysis/
```

### Step 7: Final Validation and Commit

```bash
# Exit container
exit

# Validate complete workflow
make docker-check-renv-fix    # Update dependencies
make docker-test             # Run comprehensive tests
make docker-render           # Test report generation

# Final commit
git add .
git commit -m "Complete Palmer Penguins analysis workflow

ANALYSIS RESULTS:
- Species classification accuracy: >95% using Random Forest
- Clear bill dimension clustering by species
- Adelie: shorter, deeper bills (18mm depth, 38mm length)
- Gentoo: longest, shallowest bills (14mm depth, 47mm length)
- Chinstrap: intermediate morphology

TECHNICAL IMPLEMENTATION:
- Systematic exploratory data analysis with quality checks
- tidymodels classification pipeline with hyperparameter tuning
- Comprehensive testing framework (22 tests, 100% coverage)
- Publication-quality visualizations and interactive plots
- Complete reproducibility documentation

FILES CREATED:
- 8 high-resolution figures (PNG + PDF)
- 4 summary tables (CSV format)
- Interactive dashboard components
- Trained classification model (RDS)
- Comprehensive session logs

All dependencies tracked, tests passing, fully reproducible."

git push origin main
```

## Demonstrated Methodological Features

This workflow implementation demonstrates the analysis paradigm's methodological capabilities:

**1. Systematic Approach:**
- Structured templates guide comprehensive analysis
- Quality checks built into every step
- Reproducible seed management throughout

**2. Research-Standard Output:**
- Publication-grade visualizations
- Interactive analytical tools
- Comprehensive statistical validation procedures

**3. Methodological Reproducibility:**
- Containerized environment maintains computational consistency
- Session logging provides comprehensive analytical documentation
- Dependency management prevents version drift

**4. Collaborative Infrastructure:**
- Version control integration with continuous integration workflows
- Standardized project architecture facilitates collaboration
- Comprehensive documentation standards

**5. Quality Assurance Framework:**
- Systematic function testing protocols
- Data validation procedures
- Integration testing across analytical workflows

## Advanced Features

### Using Additional Variants

```bash
# Add specialized environments for different needs
./add_environment.sh

# Select environments like:
# - modeling: Enhanced ML packages (xgboost, h2o, keras)
# - alpine_minimal: Lightweight for CI/CD (~200MB)
# - publishing: LaTeX support for manuscript preparation
```

### Interactive Dashboards

The analysis paradigm includes `04_interactive_dashboard.Rmd` for creating Shiny applications:

```r
# Interactive exploration dashboard
library(shiny)
library(DT)
library(plotly)

# Load model and create real-time species prediction interface
# Full template available in scripts/04_interactive_dashboard.Rmd
```

### Automated Reporting

The `05_automated_report.Rmd` template supports parameterized reports:

```r
# Generate reports for different species or time periods
rmarkdown::render("scripts/05_automated_report.Rmd",
                  params = list(species = "Adelie",
                               year_range = c(2007, 2009)))
```

## Transitioning from Solo to Team Development

Your solo analysis project is automatically team-ready! When you're ready to collaborate:

### 1. Add Docker Hub Configuration
```bash
# Configure team sharing (one-time setup)
zzcollab --config set team-name "your-dockerhub-account"

# Create shareable team images for existing project
cd your-project
zzcollab -i -t your-team -p your-project -B rstudio
```

### 2. Team Members Join
```bash
# Team members can immediately join your project
git clone https://github.com/your-team/your-project.git
cd your-project
zzcollab -t your-team -p your-project -I rstudio

# Equivalent development environment
make docker-zsh
```

The zzcollab framework enables solo projects to operate without Docker Hub configuration while maintaining compatibility for team collaboration when required.

## Characteristics of Analysis Paradigm Workflow

The analysis paradigm demonstrates the following methodological characteristics:

- **Systematic Structure**: Six-phase template sequence provides comprehensive analytical coverage
- **Reproducible Methods**: Integrated seed management and session documentation ensure replicability
- **Research-Grade Output**: Publication-standard visualizations and standardized reporting formats
- **Quality Assurance**: Comprehensive testing framework with coverage targets exceeding 90%
- **Collaborative Scalability**: Architecture supports transition from individual to team-based development
- **Resource Efficiency**: Pre-configured package environments reduce initialization overhead
- **Technical Flexibility**: Multiple Docker environments accommodate diverse computational requirements
- **Contemporary Standards**: Integration with current R ecosystem best practices (tidyverse, tidymodels)

The analysis paradigm provides a complete framework for reproducible data science, from initial data exploration through final publication-ready results.

## Complete Practical Example: Iterative Penguin Bill Analysis

This section demonstrates a complete iterative development workflow using the Palmer penguins dataset, showing how to build analysis incrementally with proper testing and documentation.

### Step 1: Create Project and Initial Analysis

```bash
# Set up the project with analysis paradigm
zzcollab -i -p penguin-analysis -P analysis --github

# Start development environment
cd penguin-analysis
make docker-zsh
```

**Inside the container - Create initial analysis script:**

```bash
# Create the analysis script
vim scripts/01_penguin_exploration.R
```

**Contents of `scripts/01_penguin_exploration.R`:**

```r
#' Penguin Bill Analysis
#' Explore relationship between bill depth and log of bill length

# Load required packages
library(palmerpenguins)
library(ggplot2)
library(dplyr)

#' Create scatter plot of bill depth vs log(bill length)
#' @return ggplot object
create_bill_plot <- function() {
  penguins %>%
    filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    ggplot(aes(x = log(bill_length_mm), y = bill_depth_mm)) +
    geom_point(aes(color = species), alpha = 0.7, size = 2) +
    labs(
      title = "Penguin Bill Depth vs Log(Bill Length)",
      x = "Log(Bill Length) (mm)",
      y = "Bill Depth (mm)",
      color = "Species"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# Create and display the plot
bill_plot <- create_bill_plot()
print(bill_plot)

# Save the plot
ggsave("figures/bill_analysis.png", bill_plot, width = 8, height = 6, dpi = 300)

cat("Analysis complete! Plot saved to figures/bill_analysis.png\n")
```

**Create the function file:**

```bash
# Create R function file
vim R/penguin_functions.R
```

**Contents of `R/penguin_functions.R`:**

```r
#' Create scatter plot of bill depth vs log(bill length)
#'
#' @param data Data frame containing penguin data (default: palmerpenguins::penguins)
#' @return ggplot object
#' @export
#' @examples
#' plot <- create_bill_plot()
#' print(plot)
create_bill_plot <- function(data = palmerpenguins::penguins) {
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("ggplot2 package is required")
  }
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("dplyr package is required")
  }

  data %>%
    dplyr::filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    ggplot2::ggplot(ggplot2::aes(x = log(bill_length_mm), y = bill_depth_mm)) +
    ggplot2::geom_point(ggplot2::aes(color = species), alpha = 0.7, size = 2) +
    ggplot2::labs(
      title = "Penguin Bill Depth vs Log(Bill Length)",
      x = "Log(Bill Length) (mm)",
      y = "Bill Depth (mm)",
      color = "Species"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom")
}
```

**Create comprehensive tests:**

```bash
# Create test file
vim tests/testthat/test-penguin_functions.R
```

**Contents of `tests/testthat/test-penguin_functions.R`:**

```r
test_that("create_bill_plot works correctly", {
  # Test with default data
  plot <- create_bill_plot()

  # Check that it returns a ggplot object
  expect_s3_class(plot, "ggplot")

  # Check plot components
  expect_equal(plot$labels$title, "Penguin Bill Depth vs Log(Bill Length)")
  expect_equal(plot$labels$x, "Log(Bill Length) (mm)")
  expect_equal(plot$labels$y, "Bill Depth (mm)")
  expect_equal(plot$labels$colour, "Species")
})

test_that("create_bill_plot handles custom data", {
  # Create test data
  test_data <- data.frame(
    bill_length_mm = c(40, 45, 50),
    bill_depth_mm = c(18, 20, 22),
    species = c("A", "B", "C")
  )

  plot <- create_bill_plot(test_data)
  expect_s3_class(plot, "ggplot")
})

test_that("create_bill_plot handles missing values", {
  # Create test data with NA values
  test_data <- data.frame(
    bill_length_mm = c(40, NA, 50),
    bill_depth_mm = c(18, 20, NA),
    species = c("A", "B", "C")
  )

  plot <- create_bill_plot(test_data)
  expect_s3_class(plot, "ggplot")

  # Should have only 1 point after filtering NAs
  expect_equal(nrow(plot$data), 1)
})
```

**Test and run the initial analysis:**

```bash
# Install required packages (if not already installed)
R
install.packages(c("palmerpenguins", "ggplot2", "dplyr"))
quit()

# Test the function
R
devtools::load_all()
devtools::test()
quit()

# Run the analysis script
mkdir -p figures
R --vanilla < scripts/01_penguin_exploration.R

# Check the results
git status
git add .
git diff --cached
```

### Step 2: Exit Container and First Commit

```bash
# Exit the development container
exit

# Validate dependencies and test in clean environment
make docker-check-renv-fix    # Auto-add new packages to renv.lock
make docker-test             # Run tests in clean environment

# First commit and push
git add .
git commit -m "Add initial penguin bill analysis

- Create scatter plot of bill depth vs log(bill length)
- Add create_bill_plot() function with comprehensive tests
- Generate publication-quality figure
- All tests passing, dependencies tracked"

git push origin main
```

### Step 3: Enhance Analysis - Add Regression Line

After the first push, continue with enhanced analysis:

```bash
# Start development environment again
make docker-zsh

# Update the analysis script
vim scripts/01_penguin_exploration.R
```

**Enhanced `scripts/01_penguin_exploration.R`:**

```r
#' Penguin Bill Analysis - Enhanced with Regression
#' Explore relationship between bill depth and log of bill length

# Load required packages
library(palmerpenguins)
library(ggplot2)
library(dplyr)
library(broom)

#' Create enhanced scatter plot with regression line
#' @return ggplot object
create_enhanced_bill_plot <- function() {
  penguins %>%
    filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    ggplot(aes(x = log(bill_length_mm), y = bill_depth_mm)) +
    geom_point(aes(color = species), alpha = 0.7, size = 2) +
    geom_smooth(method = "lm", se = TRUE, color = "black", linetype = "dashed") +
    labs(
      title = "Penguin Bill Depth vs Log(Bill Length) with Regression Line",
      x = "Log(Bill Length) (mm)",
      y = "Bill Depth (mm)",
      color = "Species",
      caption = "Dashed line shows linear regression fit with 95% confidence interval"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}

#' Fit linear model for bill depth vs log(bill length)
#' @return list with model object and summary statistics
fit_bill_model <- function() {
  clean_data <- penguins %>%
    filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    mutate(log_bill_length = log(bill_length_mm))

  model <- lm(bill_depth_mm ~ log_bill_length, data = clean_data)

  list(
    model = model,
    summary = summary(model),
    r_squared = summary(model)$r.squared,
    coefficients = tidy(model)
  )
}

# Create enhanced plot
enhanced_plot <- create_enhanced_bill_plot()
print(enhanced_plot)

# Fit regression model
model_results <- fit_bill_model()
cat("\nRegression Results:\n")
cat("R-squared:", round(model_results$r_squared, 3), "\n")
print(model_results$coefficients)

# Save outputs
ggsave("figures/bill_analysis_with_regression.png", enhanced_plot,
       width = 8, height = 6, dpi = 300)

# Save model results
saveRDS(model_results, "results/bill_model.rds")

cat("\nEnhanced analysis complete!\n")
cat("Plot: figures/bill_analysis_with_regression.png\n")
cat("Model: results/bill_model.rds\n")
```

**Update function file with enhanced functions:**

```r
# Add to R/penguin_functions.R

#' Create enhanced scatter plot with regression line
#'
#' @param data Data frame containing penguin data (default: palmerpenguins::penguins)
#' @return ggplot object
#' @export
create_enhanced_bill_plot <- function(data = palmerpenguins::penguins) {
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("ggplot2 package is required")
  }
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("dplyr package is required")
  }

  data %>%
    dplyr::filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    ggplot2::ggplot(ggplot2::aes(x = log(bill_length_mm), y = bill_depth_mm)) +
    ggplot2::geom_point(ggplot2::aes(color = species), alpha = 0.7, size = 2) +
    ggplot2::geom_smooth(method = "lm", se = TRUE, color = "black", linetype = "dashed") +
    ggplot2::labs(
      title = "Penguin Bill Depth vs Log(Bill Length) with Regression Line",
      x = "Log(Bill Length) (mm)",
      y = "Bill Depth (mm)",
      color = "Species",
      caption = "Dashed line shows linear regression fit with 95% confidence interval"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom")
}

#' Fit linear model for bill depth vs log(bill length)
#'
#' @param data Data frame containing penguin data (default: palmerpenguins::penguins)
#' @return list with model object and summary statistics
#' @export
fit_bill_model <- function(data = palmerpenguins::penguins) {
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("dplyr package is required")
  }
  if (!requireNamespace("broom", quietly = TRUE)) {
    stop("broom package is required")
  }

  clean_data <- data %>%
    dplyr::filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
    dplyr::mutate(log_bill_length = log(bill_length_mm))

  model <- lm(bill_depth_mm ~ log_bill_length, data = clean_data)

  list(
    model = model,
    summary = summary(model),
    r_squared = summary(model)$r.squared,
    coefficients = broom::tidy(model)
  )
}
```

**Add comprehensive tests for enhanced functions:**

```r
# Add to tests/testthat/test-penguin_functions.R

test_that("create_enhanced_bill_plot works correctly", {
  plot <- create_enhanced_bill_plot()

  expect_s3_class(plot, "ggplot")
  expect_equal(plot$labels$title,
               "Penguin Bill Depth vs Log(Bill Length) with Regression Line")
  expect_true(grepl("regression", plot$labels$caption, ignore.case = TRUE))
})

test_that("fit_bill_model returns correct structure", {
  model_results <- fit_bill_model()

  expect_type(model_results, "list")
  expect_true("model" %in% names(model_results))
  expect_true("summary" %in% names(model_results))
  expect_true("r_squared" %in% names(model_results))
  expect_true("coefficients" %in% names(model_results))

  expect_s3_class(model_results$model, "lm")
  expect_type(model_results$r_squared, "double")
  expect_s3_class(model_results$coefficients, "data.frame")
})

test_that("fit_bill_model handles custom data", {
  test_data <- data.frame(
    bill_length_mm = c(40, 45, 50, 55),
    bill_depth_mm = c(18, 19, 20, 21),
    species = c("A", "B", "C", "A")
  )

  model_results <- fit_bill_model(test_data)
  expect_s3_class(model_results$model, "lm")
  expect_true(model_results$r_squared >= 0 && model_results$r_squared <= 1)
})
```

**Test and run the enhanced analysis:**

```bash
# Install new package
R
install.packages("broom")
quit()

# Test the new functions
R
devtools::load_all()
devtools::test()
quit()

# Create results directory and run enhanced analysis
mkdir -p results
R --vanilla < scripts/01_penguin_exploration.R

# Check what changed
git status
git diff
```

### Step 4: Second Commit with Enhancement

```bash
# Exit container
exit

# Validate enhanced analysis
make docker-check-renv-fix    # Track new broom package
make docker-test             # Ensure all tests pass

# Commit the enhancement
git add .
git commit -m "Add regression analysis to penguin bill study

- Add linear regression line to scatter plot
- Implement fit_bill_model() function with model diagnostics
- Include R-squared and coefficient estimates
- Add comprehensive tests for regression functionality
- Save model results for reproducibility
- All tests passing, broom package added to dependencies"

git push origin main
```

## The Critical Importance of Data Unit Testing

One of the most underappreciated aspects of reproducible data analysis is the inclusion of comprehensive unit tests for both raw and processed datasets. The Palmer penguins example above demonstrates this crucial practice.

### Why Data Unit Testing Matters

**Prevents Silent Data Corruption**: Data issues often manifest as subtle changes that don't produce errors but invalidate results:
- Inadvertent filtering that removes important observations
- Calculation errors in derived variables
- Data import issues that change variable types
- Missing value handling that introduces bias

**Ensures Reproducibility**: Tests document expected data properties:
- Future collaborators understand data assumptions
- Changes in upstream data sources are immediately detected
- Data processing pipelines can be validated after modifications
- Quality standards are explicitly defined and maintained

**Saves Time and Prevents Errors**: Early detection of data issues prevents:
- Hours of debugging downstream analysis problems
- Publishing results based on corrupted data
- Having to retract findings due to data errors
- Confusion about why models suddenly perform differently

### Data Testing Best Practices Demonstrated

**1. Raw Data Validation (Pre-Processing)**
```r
# Structural validation
test_that("Palmer penguins raw data has expected structure", {
  expect_equal(nrow(penguins_raw), 344)   # Known dataset size
  expect_equal(ncol(penguins_raw), 8)     # Expected variables
})

# Biological range validation
test_that("Measurements are within biologically plausible ranges", {
  expect_true(all(bill_lengths >= 30 & bill_lengths <= 70))
  expect_true(all(body_masses >= 2000 & body_masses <= 7000))
})

# Categorical value validation
test_that("Species values are from known set", {
  expect_true(all(species %in% c("Adelie", "Chinstrap", "Gentoo")))
})
```

**2. Processed Data Validation (Post-Processing)**
```r
# Data retention validation
test_that("Data cleaning preserved data integrity", {
  retention_rate <- nrow(clean_data) / nrow(raw_data)
  expect_true(retention_rate >= 0.95)  # Should retain ≥95%
})

# Derived variable validation
test_that("Calculated variables are mathematically correct", {
  expected_ratios <- length_mm / depth_mm
  expect_equal(bill_ratio, expected_ratios)
})

# Relationship preservation
test_that("Known relationships are maintained", {
  correlation <- cor(body_mass, flipper_length)
  expect_true(correlation > 0)  # Should remain positive
})
```

**3. File System Validation**
```r
# Export integrity validation
test_that("Processed data saved correctly", {
  expect_true(file.exists("data/processed/penguins_clean.csv"))
  reloaded_data <- read_csv("data/processed/penguins_clean.csv")
  expect_equal(nrow(reloaded_data), nrow(processed_data))
})
```

### Integration with Analysis Workflow

**Embedding Tests in Analysis Scripts**: The example demonstrates how to include data tests directly in exploratory analysis scripts, making validation an integral part of the analysis process rather than an afterthought.

**Automated Test Reporting**: Creating structured test reports (`data_testing_report.rds`) provides:
- Documentation of all validation checks performed
- Clear pass/fail status for each test category
- Timestamp information for reproducibility tracking
- Metadata about data transformations and retention rates

**Version Control Integration**: Data tests should be committed alongside analysis code, creating a permanent record of data quality expectations and validation procedures.

### Long-term Benefits

**1. Maintainable Research**: As projects evolve, data tests ensure that modifications don't break existing analyses or introduce subtle errors.

**2. Collaborative Confidence**: Team members can work with processed data knowing it has passed comprehensive validation checks.

**3. Publication Integrity**: Journals increasingly require evidence of computational reproducibility - comprehensive data testing provides this evidence.

**4. Methodological Transparency**: Tests document assumptions about data quality and acceptable ranges, making methodology more transparent.

### Implementing Data Testing in Your Projects

**Start Simple**: Begin with basic structural tests (dimensions, column names, data types) and gradually add domain-specific validation.

**Be Comprehensive**: Test both raw and processed data at every major transformation step.

**Document Expectations**: Use informative test messages that explain why each check is important.

**Automate Validation**: Include data tests in your standard analysis workflow so they run automatically.

**Version Control Tests**: Commit test code alongside analysis code to maintain the complete reproducibility story.

Data unit testing transforms data analysis from a fragile, error-prone process into a robust, reliable methodology that produces trustworthy results suitable for scientific publication and practical application.

## CI/CD Workflow Construction for Reproducible Analysis

Continuous Integration and Continuous Deployment (CI/CD) workflows are essential for maintaining analysis quality and reproducibility. ZZCOLLAB automatically generates GitHub Actions workflows tailored to your analysis paradigm, but understanding how to construct and customize these workflows is crucial for advanced analysis projects.

### Understanding ZZCOLLAB's Generated CI/CD Workflows

When you create a project with `zzcollab -p penguin-analysis -P analysis --github`, ZZCOLLAB automatically creates `.github/workflows/analysis-paradigm.yml`:

```yaml
# .github/workflows/analysis-paradigm.yml
# Auto-generated by ZZCOLLAB for analysis paradigm projects
name: Analysis Paradigm CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly to catch upstream dependency issues
    - cron: '0 10 * * 1'  # Monday 10:00 UTC

jobs:
  analysis-validation:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        r-version: ['4.3', '4.4', 'release']
        include:
          - r-version: 'release'
            check-full: true

    steps:
    - uses: actions/checkout@v4

    - name: Set up R ${{ matrix.r-version }}
      uses: r-lib/actions/setup-r@v2
      with:
        r-version: ${{ matrix.r-version }}
        use-public-rspm: true

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev

    - name: Cache R packages
      uses: actions/cache@v3
      with:
        path: ~/.local/share/renv
        key: ${{ runner.os }}-renv-${{ hashFiles('**/renv.lock') }}
        restore-keys: ${{ runner.os }}-renv-

    - name: Restore renv environment
      run: |
        R -e "install.packages('renv')"
        R -e "renv::restore()"

    - name: Run data unit tests
      run: |
        R -e "library(testthat); source('scripts/01_exploratory_analysis.R')"

    - name: Run R package tests
      run: R -e "devtools::test()"

    - name: Check R package
      if: matrix.check-full
      run: R -e "rcmdcheck::rcmdcheck(args = '--no-manual', error_on = 'warning')"

    - name: Validate dependency consistency
      run: R -e "source('check_renv_for_commit.R')"

    - name: Generate analysis reports
      run: |
        R -e "rmarkdown::render('scripts/05_automated_report.Rmd')"

    - name: Upload analysis artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: analysis-results-r${{ matrix.r-version }}
        path: |
          outputs/
          reports/
          scripts/*.html
```

### 1. GitHub Actions Workflow Explanation

**Workflow Structure Components:**

**Triggers (`on`)**: Define when the workflow runs
- **Push events**: Runs on main/develop branch pushes
- **Pull requests**: Validates changes before merging
- **Scheduled runs**: Weekly dependency checks to catch upstream issues

**Matrix Strategy**: Tests across multiple R versions
- **Version coverage**: R 4.3, 4.4, and latest release
- **Conditional logic**: Full package checks only on latest R version
- **Fail-fast disabled**: Allows seeing results across all R versions

**Step Progression**: Logical validation sequence
1. **Environment setup**: R installation and system dependencies
2. **Dependency management**: renv restoration with caching
3. **Data validation**: Run data unit tests from analysis scripts
4. **Code validation**: Standard R package testing
5. **Report generation**: Automated analysis output creation
6. **Artifact preservation**: Save results for download/inspection

### 2. Workflow Customization for Analysis Projects

**Adding Analysis-Specific Validation Steps:**

```yaml
    - name: Validate Palmer Penguins data integrity
      run: |
        R -e "
        source('R/data_testing_functions.R')
        library(testthat)

        # Test raw data availability and integrity
        if (!file.exists('data/raw/penguins.csv')) {
          stop('Raw penguins data not found')
        }

        # Run comprehensive data tests
        penguins_raw <- readr::read_csv('data/raw/penguins.csv')
        validate_penguin_data_structure(penguins_raw)
        validate_penguin_data_ranges(penguins_raw)
        validate_penguin_categorical_values(penguins_raw)

        cat('✅ All Palmer Penguins data validation tests passed\n')
        "

    - name: Run complete analysis pipeline
      run: |
        R -e "
        # Execute complete analysis in order
        message('Running exploratory analysis...')
        source('scripts/01_exploratory_analysis.R')

        message('Running statistical modeling...')
        source('scripts/02_statistical_modeling.R')

        message('Running model validation...')
        source('scripts/03_model_validation.R')

        # Verify all expected outputs were created
        expected_files <- c(
          'outputs/figures/comprehensive_morphometrics.png',
          'outputs/models/penguin_classification_models.rds',
          'analysis/exploratory/quality_report.rds',
          'analysis/exploratory/data_testing_report.rds'
        )

        missing_files <- expected_files[!file.exists(expected_files)]
        if (length(missing_files) > 0) {
          stop('Missing expected output files: ', paste(missing_files, collapse = ', '))
        }

        cat('✅ Complete analysis pipeline executed successfully\n')
        "

    - name: Validate model performance standards
      run: |
        R -e "
        # Load model results and validate performance
        model_results <- readRDS('outputs/models/penguin_classification_models.rds')

        # Set performance thresholds for Palmer Penguins
        min_accuracy <- 0.85  # Species classification should be ≥85% accurate
        min_roc_auc <- 0.90   # ROC-AUC should be ≥0.90

        # Check best model performance
        best_performance <- model_results$performance_summary$best_model

        if (best_performance$accuracy < min_accuracy) {
          stop('Model accuracy (', round(best_performance$accuracy, 3),
               ') below threshold (', min_accuracy, ')')
        }

        if (best_performance$roc_auc < min_roc_auc) {
          stop('Model ROC-AUC (', round(best_performance$roc_auc, 3),
               ') below threshold (', min_roc_auc, ')')
        }

        cat('✅ Model performance meets quality standards\n')
        cat('   Accuracy:', round(best_performance$accuracy, 3), '\n')
        cat('   ROC-AUC:', round(best_performance$roc_auc, 3), '\n')
        "
```

### 3. Custom Validation Steps Integration

**Domain-Specific Validation for Palmer Penguins Analysis:**

```yaml
    - name: Biological validation checks
      run: |
        R -e "
        # Load processed data
        penguins_clean <- readr::read_csv('data/processed/penguins_clean.csv')

        # Validate species-specific biological constraints
        library(dplyr)

        # Check Adelie penguin characteristics (known to be smallest)
        adelie_data <- filter(penguins_clean, species == 'Adelie')
        gentoo_data <- filter(penguins_clean, species == 'Gentoo')

        # Adelie should have shorter bills than Gentoo on average
        adelie_mean_bill <- mean(adelie_data$bill_length_mm)
        gentoo_mean_bill <- mean(gentoo_data$bill_length_mm)

        if (adelie_mean_bill >= gentoo_mean_bill) {
          warning('Unexpected: Adelie bill length ≥ Gentoo bill length')
        }

        # Check sexual dimorphism patterns
        sexual_dimorphism <- penguins_clean %>%
          filter(!is.na(sex)) %>%
          group_by(species, sex) %>%
          summarise(mean_mass = mean(body_mass_g), .groups = 'drop') %>%
          pivot_wider(names_from = sex, values_from = mean_mass) %>%
          mutate(dimorphism_ratio = male / female)

        # Males should be heavier than females (penguin biology)
        if (any(sexual_dimorphism$dimorphism_ratio < 1)) {
          stop('Biological validation failed: Males lighter than females')
        }

        cat('✅ Biological validation checks passed\n')
        "

    - name: Statistical significance validation
      run: |
        R -e "
        # Validate that key statistical findings are significant

        # Load analysis results
        penguins_clean <- readr::read_csv('data/processed/penguins_clean.csv')

        # Test species differences in bill dimensions (should be highly significant)
        bill_length_anova <- aov(bill_length_mm ~ species, data = penguins_clean)
        bill_depth_anova <- aov(bill_depth_mm ~ species, data = penguins_clean)

        length_p <- summary(bill_length_anova)[[1]]$'Pr(>F)'[1]
        depth_p <- summary(bill_depth_anova)[[1]]$'Pr(>F)'[1]

        # Palmer Penguins should show highly significant species differences
        if (length_p > 0.001) {
          stop('Bill length species differences not significant (p = ',
               format(length_p, scientific = TRUE), ')')
        }

        if (depth_p > 0.001) {
          stop('Bill depth species differences not significant (p = ',
               format(depth_p, scientific = TRUE), ')')
        }

        cat('✅ Statistical significance validation passed\n')
        cat('   Bill length ANOVA p-value:', format(length_p, scientific = TRUE), '\n')
        cat('   Bill depth ANOVA p-value:', format(depth_p, scientific = TRUE), '\n')
        "
```

### 4. Multi-Environment Testing Strategy

**Testing Across Different R Versions and Package Combinations:**

```yaml
  multi-environment-testing:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macOS-latest]
        r-version: ['4.3', '4.4', 'release']
        build-mode: ['fast', 'standard', 'comprehensive']
        include:
          # Test minimal environment
          - os: ubuntu-latest
            r-version: 'release'
            build-mode: 'fast'
            test-type: 'minimal'

          # Test full environment
          - os: ubuntu-latest
            r-version: 'release'
            build-mode: 'comprehensive'
            test-type: 'full'

          # Test legacy R version compatibility
          - os: ubuntu-latest
            r-version: '4.2'
            build-mode: 'standard'
            test-type: 'legacy'

    steps:
    - uses: actions/checkout@v4

    - name: Set up R ${{ matrix.r-version }}
      uses: r-lib/actions/setup-r@v2
      with:
        r-version: ${{ matrix.r-version }}

    - name: Install build-mode specific packages
      run: |
        R -e "
        # Install packages based on build mode
        build_mode <- '${{ matrix.build-mode }}'

        if (build_mode == 'fast') {
          packages <- c('renv', 'here', 'usethis', 'devtools', 'testthat', 'knitr', 'rmarkdown', 'palmerpenguins')
        } else if (build_mode == 'standard') {
          packages <- c('renv', 'here', 'usethis', 'devtools', 'dplyr', 'ggplot2', 'tidyr', 'testthat', 'palmerpenguins', 'broom', 'janitor', 'DT', 'conflicted', 'knitr', 'rmarkdown')
        } else {
          # comprehensive mode - install all packages
          renv::restore()
          return()
        }

        install.packages(packages, dependencies = TRUE)
        "

    - name: Test analysis with specific environment
      run: |
        R -e "
        # Test core functionality with limited package set
        cat('Testing with build mode: ${{ matrix.build-mode }}\n')

        # Basic data loading and processing should work in all modes
        library(palmerpenguins)
        data(penguins)

        # Basic analysis that should work in all build modes
        library(dplyr)
        basic_summary <- penguins %>%
          filter(!is.na(bill_length_mm), !is.na(species)) %>%
          group_by(species) %>%
          summarise(
            n = n(),
            mean_bill_length = mean(bill_length_mm),
            .groups = 'drop'
          )

        # Validate basic results
        if (nrow(basic_summary) != 3) {
          stop('Expected 3 species, got ', nrow(basic_summary))
        }

        cat('✅ Basic analysis works with ${{ matrix.build-mode }} mode\n')
        print(basic_summary)
        "
```

### 5. Automated Reporting in CI/CD

**Generating and Deploying Analysis Reports:**

```yaml
    - name: Generate comprehensive analysis report
      run: |
        R -e "
        # Generate timestamped analysis report
        timestamp <- format(Sys.time(), '%Y%m%d_%H%M%S')

        # Create comprehensive report
        rmarkdown::render(
          'scripts/05_automated_report.Rmd',
          output_file = paste0('palmer_penguins_analysis_', timestamp, '.html'),
          output_dir = 'reports/',
          params = list(
            include_all_models = TRUE,
            include_diagnostics = TRUE,
            include_data_quality = TRUE,
            git_commit = system('git rev-parse HEAD', intern = TRUE),
            r_version = R.version.string,
            analysis_date = Sys.Date()
          )
        )

        # Generate model performance summary
        cat('# Palmer Penguins Analysis Summary\n\n', file = 'reports/ANALYSIS_SUMMARY.md')
        cat('Generated:', format(Sys.time()), '\n', file = 'reports/ANALYSIS_SUMMARY.md', append = TRUE)
        cat('Git commit:', system('git rev-parse HEAD', intern = TRUE), '\n', file = 'reports/ANALYSIS_SUMMARY.md', append = TRUE)
        "

    - name: Deploy reports to GitHub Pages
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Configure git for automated deployment
        git config --local user.email 'action@github.com'
        git config --local user.name 'GitHub Action'

        # Create or update gh-pages branch
        git checkout --orphan gh-pages
        git rm -rf .

        # Copy reports to root
        cp -r reports/* .
        cp reports/palmer_penguins_analysis_*.html index.html

        # Create simple index page
        echo "# Palmer Penguins Analysis Reports" > README.md
        echo "Latest analysis report: [index.html](index.html)" >> README.md

        git add .
        git commit -m "Deploy analysis reports"
        git push --force --set-upstream origin gh-pages

    - name: Create analysis summary comment (for PRs)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          // Read analysis summary
          const summary = fs.readFileSync('reports/ANALYSIS_SUMMARY.md', 'utf8');

          // Create comment body
          const commentBody = `## 🐧 Palmer Penguins Analysis Results

          ${summary}

          ### Key Metrics
          - ✅ All data validation tests passed
          - ✅ Model performance above thresholds
          - ✅ Statistical significance validated
          - 📊 Full report available in artifacts

          _Analysis generated automatically by CI/CD workflow_`;

          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: commentBody
          });
```

### Best Practices for Analysis-Specific CI/CD Customization

**1. Domain-Aware Validation**
- **Biological constraints**: Test that results align with known biology (e.g., sexual dimorphism patterns)
- **Statistical expectations**: Validate that key effects are statistically significant
- **Data quality thresholds**: Set minimum standards for data retention and completeness

**2. Performance Benchmarking**
- **Model accuracy thresholds**: Set minimum performance standards for your domain
- **Computational efficiency**: Monitor analysis runtime and memory usage
- **Reproducibility checks**: Ensure identical results across runs with same data

**3. Automated Quality Gates**
- **Failed analysis detection**: Stop workflow if critical analysis steps fail
- **Performance degradation alerts**: Flag when model performance drops below thresholds
- **Data drift detection**: Monitor for changes in data distributions over time

**4. Comprehensive Reporting**
- **Stakeholder-friendly summaries**: Generate executive summaries alongside technical reports
- **Visual artifacts**: Automatically save and version key plots and visualizations
- **Metadata tracking**: Document analysis provenance, dependencies, and environment details

**5. Environment-Specific Testing**
- **Package compatibility**: Test across different package versions and build modes
- **Platform validation**: Ensure analysis works on Windows, macOS, and Linux
- **Scalability testing**: Validate performance with different data sizes

### Palmer Penguins Customization Example

**Complete custom workflow for Palmer Penguins analysis:**

```yaml
name: Palmer Penguins Analysis CI/CD

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  penguin-analysis:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - uses: r-lib/actions/setup-r@v2

    - name: Install dependencies
      run: |
        R -e "install.packages(c('palmerpenguins', 'dplyr', 'ggplot2', 'tidymodels', 'testthat'))"

    - name: Validate penguin data integrity
      run: R -e "source('tests/test_penguin_data_integrity.R')"

    - name: Run species classification analysis
      run: R -e "source('scripts/01_exploratory_analysis.R')"

    - name: Validate model performance
      run: R -e "source('tests/test_model_performance_thresholds.R')"

    - name: Generate analysis report
      run: R -e "rmarkdown::render('reports/penguin_analysis_report.Rmd')"

    - name: Upload analysis artifacts
      uses: actions/upload-artifact@v3
      with:
        name: penguin-analysis-results
        path: |
          outputs/
          reports/
```

This comprehensive CI/CD framework ensures that Palmer Penguins analyses maintain high quality standards, detect regressions early, and provide stakeholders with reliable, automatically-generated insights while maintaining full reproducibility and transparency.

### What This Example Demonstrates

1. **Complete Workflow**: From initial analysis to enhanced version with iterative development
2. **Professional Practices**: Functions, tests, documentation following R package standards
3. **Data Unit Testing**: Comprehensive validation of both raw and processed datasets (7 test categories)
4. **CI/CD Construction**: GitHub Actions workflows with domain-specific validation and automated reporting
5. **Iterative Development**: Building on previous work incrementally with version control
6. **Dependency Tracking**: Automatic renv.lock updates for package management
7. **Reproducible Outputs**: Saved plots and model objects for reproducibility
8. **Quality Assurance**: Comprehensive tests validate function behavior and prevent regressions
9. **Version Control**: Clear commit messages documenting detailed changes and rationale

This iterative workflow demonstrates systematic data science development using the zzcollab analysis paradigm framework.
