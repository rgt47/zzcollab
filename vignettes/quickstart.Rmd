---
title: "Quick Start: Complete Reproducible Data Workflow"
author: "ZZCOLLAB Development Team"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Quick Start: Complete Reproducible Data Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Introduction

This quick start guide demonstrates how to create a fully reproducible analysis
with all 5 pillars of reproducibility. You will create a complete data workflow
from raw data through processing, analysis, testing, and reporting using Palmer
Penguins bill measurements.

**What you will build:**
- Complete data workflow: raw data â†’ processing â†’ analysis â†’ visualization
- All 5 pillars: Dockerfile + renv + .Rprofile + Source Code + Research Data
- Functions, tests, and documentation following best practices
- Private GitHub repository with automated CI/CD validation

**Prerequisites:**
- Docker installed and running
- GitHub account (example: rgt47)
- Personal access token for GitHub (with repo permissions)
- zzcollab installed (`./install.sh`)

# Step-by-Step Instructions

## Step 1: Configure ZZCOLLAB

Set your defaults once to eliminate repetitive typing for ALL future projects:

```bash
# Initialize configuration
zzcollab --config init

# Set team name (typically your GitHub username for solo projects)
zzcollab --config set team-name "rgt47"

# Set GitHub account
zzcollab --config set github-account "rgt47"

# Set Docker Hub account (defaults to team-name if not set)
zzcollab --config set dockerhub-account "rgt47"

# Set default Docker profile (optional - leave empty for minimal)
zzcollab --config set profile-name ""
```

**What this accomplishes:**
- All future `zzcollab` commands use these defaults
- No need to specify `-t` flag repeatedly
- Change defaults anytime with `zzcollab --config set`

**Configuration Options Explained:**

- **profile-name**: Controls Docker environment (team/shared)
  - 14+ predefined profiles available
  - Typically specified with `-r` flag when creating projects
  - Leave empty for minimal profile (default)
  - Can also compose custom environments with `-b`, `-l`, `-k` flags

**Note on Package Management:**
- Packages are added via standard `install.packages()` inside the container as needed
- **Auto-snapshot**: Packages automatically captured in renv.lock when you exit the container
- **Auto-restore**: Missing packages automatically installed when you start R
- No need to pre-configure a "mode" - just install packages when you need them
- The renv.lock accumulates packages from all team members

## Step 2: Create Project

Create your reproducible analysis project:

```bash
# Create project directory and initialize
mkdir penguin-bills && cd penguin-bills
zzcollab

# Flags come from config (set in Step 1):
# -t rgt47: from team-name config

# Build Docker image (required step)
make docker-build
```

**What this does:**
1. `zzcollab`: Creates complete project structure (R/, analysis/, tests/, .github/, etc.)
2. `zzcollab`: Sets up CI/CD workflows for automated testing
3. `zzcollab`: Initializes renv for dependency tracking
4. `make docker-build`: Builds Docker image with R + essential packages

**Solo Developer Simplification:**

Since you are working solo (not creating a team image), zzcollab creates a
streamlined development environment optimized for individual work.

**Using Different Docker Profiles:**

ZZCOLLAB provides 14+ predefined profiles for specialized environments. Use the `-r` flag to specify a profile:

```bash
# Option 1: Use -r flag for profile selection (most common)
mkdir genomics-study && cd genomics-study
zzcollab -r bioinformatics
make docker-build

# Option 2: Use -r flag for geospatial work
mkdir spatial-analysis && cd spatial-analysis
zzcollab -r geospatial
make docker-build

# Option 3: Custom composition with base image and bundles
mkdir custom-project && cd custom-project
zzcollab -b rocker/r-ver -l geospatial -k modeling
make docker-build

# Browse all available profiles
zzcollab --list-profiles  # Show all 14+ predefined profiles
zzcollab --list-libs      # System library bundles
zzcollab --list-pkgs      # R package bundles
```

**How Docker Profiles Work:**

- **Docker Profile** (`-r` flag): Controls the base Docker image and pre-installed packages
  - Shared across team members
  - 14+ predefined profiles: minimal, analysis, modeling, bioinformatics, geospatial, and more
  - Specify per project with `-r` flag (e.g., `zzcollab -r bioinformatics`)

- **Package Management**: Packages are added dynamically as needed
  - Use standard `install.packages("package-name")` inside the container
  - **Auto-snapshot**: Packages captured in renv.lock when you exit container
  - **Auto-restore**: Missing packages installed automatically when you start R
  - The renv.lock file accumulates packages from all team members
  - No need to pre-configure a "mode" - just install what you need

## Step 3: Set Repository to Private

```bash
# Already in penguin-bills/ directory from Step 2
# Make repository private using GitHub CLI
gh repo edit --visibility private
```

## Step 4: Add Research Data and Processing Pipeline

This step demonstrates the complete data workflow following the Five Pillars of Reproducibility:
- **Pillar 5**: Raw data in `analysis/data/raw_data/`
- **Pillar 4**: Processing scripts in `analysis/scripts/`
- **Pillar 4**: Reusable functions in `R/`
- **Pillar 4**: Unit tests in `tests/testthat/`

Start the Docker development environment:

```bash
# Enter Docker container
make r
```

### Add Raw Data

Create a raw data file (simulating field-collected penguin measurements).

Create `analysis/data/raw_data/penguins_raw.csv`:

```csv
species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex,year
Adelie,Torgersen,39.1,18.7,181,3750,male,2007
Adelie,Torgersen,39.5,17.4,186,3800,female,2007
Adelie,Torgersen,40.3,18.0,195,3250,female,2007
Adelie,Torgersen,NA,NA,NA,NA,NA,2007
Gentoo,Biscoe,46.1,13.2,211,4500,female,2007
Gentoo,Biscoe,50.0,16.3,230,5700,male,2007
Chinstrap,Dream,46.5,17.9,192,3500,female,2007
Chinstrap,Dream,50.0,19.5,196,3900,male,2007
```

Create `analysis/data/raw_data/README.md`:

```markdown
# Raw Data: Palmer Penguins

## Source
Simulated penguin measurements from Palmer Station Antarctica LTER.

## Collection Methods
- Field measurements of adult penguins
- Three species: Adelie, Gentoo, Chinstrap
- Three islands: Torgersen, Biscoe, Dream

## Variables
- species: Penguin species (Adelie, Gentoo, Chinstrap)
- island: Island where observed
- bill_length_mm: Bill length in millimeters
- bill_depth_mm: Bill depth in millimeters
- flipper_length_mm: Flipper length in millimeters
- body_mass_g: Body mass in grams
- sex: Penguin sex (male, female)
- year: Year of observation

## Known Issues
- Row 4 contains missing values (equipment failure)
- All measurements subject to Â±0.5mm measurement error
```

### Create Data Processing Script

Create a script to clean and process the raw data.

Create `analysis/scripts/01_process_data.R`:

```r
# Data Processing Script: Clean raw penguin data
# Input: analysis/data/raw_data/penguins_raw.csv
# Output: analysis/data/derived_data/penguins_clean.csv

library(dplyr)

# Load raw data
raw_data <- read.csv("analysis/data/raw_data/penguins_raw.csv")

# Process data
clean_data <- raw_data %>%
  # Remove rows with missing values
  filter(!is.na(bill_length_mm), !is.na(bill_depth_mm)) %>%
  # Add derived variables
  mutate(
    bill_ratio = bill_length_mm / bill_depth_mm,
    size_category = case_when(
      body_mass_g < 3500 ~ "small",
      body_mass_g < 4500 ~ "medium",
      TRUE ~ "large"
    )
  )

# Create output directory if needed
dir.create("analysis/data/derived_data", showWarnings = FALSE, recursive = TRUE)

# Save processed data
write.csv(clean_data, "analysis/data/derived_data/penguins_clean.csv", row.names = FALSE)

# Log processing results
cat("Processed", nrow(clean_data), "valid records from", nrow(raw_data), "raw records\n")
cat("Removed", nrow(raw_data) - nrow(clean_data), "incomplete records\n")
```

Create `analysis/data/derived_data/README.md`:

```markdown
# Processed Data: Palmer Penguins (Clean)

## Source
Processed from `analysis/data/raw_data/penguins_raw.csv`

## Processing Steps
See: `analysis/scripts/01_process_data.R`

1. Loaded raw data
2. Removed rows with missing bill measurements
3. Added derived variables:
   - bill_ratio: bill_length_mm / bill_depth_mm
   - size_category: small (<3500g), medium (3500-4500g), large (>4500g)

## Quality Checks
- All rows have complete bill measurements
- All derived variables computed successfully
- No outliers detected beyond expected range
```

Run the processing script:

```bash
Rscript analysis/scripts/01_process_data.R
```

### Create Reusable Analysis Functions

Create functions in `R/` directory that use the processed data.

Create `R/data_utils.R`:

```r
#' Load processed penguin data
#'
#' @param path Path to clean data file
#' @return data.frame of processed penguin data
#' @export
load_penguin_data <- function(path = "analysis/data/derived_data/penguins_clean.csv") {
  if (!file.exists(path)) {
    stop("Clean data file not found. Run analysis/scripts/01_process_data.R first.")
  }

  data <- read.csv(path, stringsAsFactors = FALSE)

  # Validate expected columns
  required_cols <- c("species", "bill_length_mm", "bill_depth_mm", "bill_ratio")
  missing_cols <- setdiff(required_cols, names(data))

  if (length(missing_cols) > 0) {
    stop("Missing required columns: ", paste(missing_cols, collapse = ", "))
  }

  data
}

#' Calculate summary statistics by species
#'
#' @param data Penguin data
#' @return data.frame of summary statistics
#' @export
summarize_by_species <- function(data) {
  data %>%
    dplyr::group_by(species) %>%
    dplyr::summarize(
      n = dplyr::n(),
      mean_bill_length = mean(bill_length_mm, na.rm = TRUE),
      mean_bill_depth = mean(bill_depth_mm, na.rm = TRUE),
      mean_bill_ratio = mean(bill_ratio, na.rm = TRUE),
      .groups = "drop"
    )
}
```

Install required packages:

```bash
# Packages will be captured in renv.lock when you exit the container
Rscript -e 'install.packages("dplyr")'
```

### Create Unit Tests

Add tests for the data processing functions.

Create `tests/testthat/test-data_utils.R`:

```r
# Source functions
source('../../R/data_utils.R')

test_that("load_penguin_data loads valid data", {
  # This test assumes processing script has been run
  skip_if_not(file.exists("analysis/data/derived_data/penguins_clean.csv"))

  data <- load_penguin_data()

  # Check structure
  expect_true(is.data.frame(data))
  expect_gt(nrow(data), 0)

  # Check required columns
  expect_true("species" %in% names(data))
  expect_true("bill_length_mm" %in% names(data))
  expect_true("bill_depth_mm" %in% names(data))
  expect_true("bill_ratio" %in% names(data))
})

test_that("load_penguin_data fails gracefully with missing file", {
  expect_error(
    load_penguin_data("/nonexistent/path.csv"),
    "Clean data file not found"
  )
})

test_that("summarize_by_species produces valid output", {
  skip_if_not(file.exists("analysis/data/derived_data/penguins_clean.csv"))

  data <- load_penguin_data()
  summary <- summarize_by_species(data)

  # Check structure
  expect_true(is.data.frame(summary))
  expect_true("species" %in% names(summary))
  expect_true("n" %in% names(summary))
  expect_true("mean_bill_length" %in% names(summary))

  # Check values are numeric
  expect_type(summary$mean_bill_length, "double")
  expect_type(summary$mean_bill_depth, "double")

  # Check all means are positive
  expect_true(all(summary$mean_bill_length > 0))
  expect_true(all(summary$mean_bill_depth > 0))
})

test_that("bill_ratio is calculated correctly", {
  skip_if_not(file.exists("analysis/data/derived_data/penguins_clean.csv"))

  data <- load_penguin_data()

  # Verify bill_ratio calculation
  expected_ratio <- data$bill_length_mm / data$bill_depth_mm
  expect_equal(data$bill_ratio, expected_ratio)
})
```

Run tests:

```bash
Rscript -e 'devtools::test()'
```

**What this accomplishes:**

1. âœ… **Raw Data** (Pillar 5): Original measurements in `analysis/data/raw_data/`
2. âœ… **Processing Pipeline**: Script in `analysis/scripts/01_process_data.R`
3. âœ… **Derived Data**: Cleaned data in `analysis/data/derived_data/`
4. âœ… **Reusable Functions**: Data utilities in `R/data_utils.R`
5. âœ… **Unit Tests**: Tests in `tests/testthat/test-data_utils.R`
6. âœ… **Documentation**: README files documenting data provenance and processing

This establishes the complete data workflow that ensures reproducibility from raw data through to analysis.

## Step 5: Create Analysis Report

Now create a report that uses the processed data and functions we created in Step 4.

### Create Visualization Function

Add a plotting function.

Create `R/bill_analysis.R`:

```r
#' Create scatter plot of bill dimensions
#'
#' @param data Penguin data with bill measurements
#' @return ggplot object
#' @export
create_bill_plot <- function(data) {
  ggplot2::ggplot(data, ggplot2::aes(x = bill_length_mm, y = bill_depth_mm,
                                      color = species)) +
    ggplot2::geom_point(size = 3, alpha = 0.7) +
    ggplot2::labs(
      title = 'Palmer Penguins: Bill Dimensions',
      x = 'Bill Length (mm)',
      y = 'Bill Depth (mm)',
      color = 'Species'
    ) +
    ggplot2::theme_minimal()
}
```

Install ggplot2:

```bash
Rscript -e 'install.packages("ggplot2")'
```

### About GUI Support (Automatic)

**The `make r` command auto-detects your profile and configures GUI support automatically:**

- **X11 profiles** (e.g., `x11_minimal`): Automatically enables DISPLAY for interactive graphics
- **RStudio profiles** (e.g., `analysis`, `publishing`): Launches RStudio Server at http://localhost:8787
- **Shiny profiles**: Launches Shiny Server at http://localhost:3838
- **Standard profiles**: Opens interactive shell

**For X11 GUI support** (if using an X11 profile):

One-time XQuartz setup (macOS):
```bash
brew install --cask xquartz
open -a XQuartz
# In Preferences > Security: Enable "Allow connections from network clients"
# Restart XQuartz, then:
export DISPLAY=:0
/opt/X11/bin/xhost +localhost
```

For this tutorial, we use the default profile and save plots to files (works in all environments including CI/CD).

### Create Analysis Report

Create the report that uses our data workflow (raw data â†’ processing â†’ functions â†’ visualization).

Create `analysis/paper/paper.Rmd`:

````
---
title: "Palmer Penguins Bill Dimensions Analysis"
author: "Reproducible Research Team"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Introduction

Analysis of bill length and bill depth measurements across three penguin species
from the Palmer Station Antarctica LTER.

This report demonstrates the complete data workflow:
- Raw data â†’ Processing â†’ Analysis â†’ Visualization

## Data Processing

```{r data-processing}
# Load data processing functions
source("../../R/data_utils.R")
source("../../R/bill_analysis.R")

library(dplyr)
library(ggplot2)

# Load the processed data
penguins <- load_penguin_data()

# Preview data
head(penguins)
```

## Summary Statistics

```{r summary-stats}
# Calculate summary by species
summary_stats <- summarize_by_species(penguins)
knitr::kable(summary_stats, digits = 2, caption = "Bill dimensions by species")
```

## Visualization

### Bill Dimensions by Species

```{r bill-plot, fig.width=8, fig.height=6}
# Generate plot using function from R/
plot <- create_bill_plot(penguins)
print(plot)

# Save plot
dir.create("../figures", showWarnings = FALSE, recursive = TRUE)
ggplot2::ggsave("../figures/bill_scatter.png", plot, width = 8, height = 6)
```

## Summary

The analysis reveals distinct bill dimension patterns across penguin species:
- Data processed from raw measurements (Step 4)
- Functions in R/ provide reusable analysis components
- Unit tests ensure correctness
````

### Render the Report

```bash
# Render the report
Rscript -e 'rmarkdown::render("analysis/paper/paper.Rmd")'

# Exit container (auto-snapshot runs here!)
exit
```

**Auto-Snapshot & Auto-Restore Architecture**

When you **exit the container**, ZZCOLLAB automatically:
1. Runs `renv::snapshot()` to capture package dependencies in renv.lock
2. Adjusts timestamp for RSPM binary package availability (10-20x faster Docker builds)
3. Validates package consistency (pure shell, no host R required!)
4. Restores timestamp to current time for accurate git history

When you **start R**, ZZCOLLAB automatically:
1. Checks if packages are missing via `renv::status()`
2. Runs `renv::restore()` to install missing packages and all dependencies
3. Ensures you always have a working environment without manual intervention

**You no longer need to manually run `renv::snapshot()` or `renv::restore()`!** Just work, exit, and restart.

## Step 6: Validate and Commit

Validate everything works in a clean environment:

```bash
# Exit the Docker container (if still inside from Step 5)
# This automatically runs renv::snapshot() and validates packages!
exit

# Run tests in clean environment
make docker-test

# Optional: Manually validate dependencies (now pure shell, no R required!)
make check-renv
```

Now commit your work:

```bash
git add .
git commit -m "$(cat <<'EOF'
Add complete data workflow: raw data to analysis

Data Workflow (5 Pillars):
- Raw data: analysis/data/raw_data/penguins_raw.csv (Pillar 5)
- Processing script: analysis/scripts/01_process_data.R (Pillar 4)
- Derived data: analysis/data/derived_data/penguins_clean.csv (Pillar 5)
- Functions: R/data_utils.R, R/bill_analysis.R (Pillar 4)
- Unit tests: tests/testthat/test-data_utils.R (Pillar 4)
- Analysis report: analysis/paper/paper.Rmd (Pillar 4)

Reproducibility:
- Pillar 1: Dockerfile (computational environment)
- Pillar 2: renv.lock (dependency tracking)
- Pillar 3: .Rprofile (R session configuration)
- Pillar 4: Source code (scripts + functions + tests + reports)
- Pillar 5: Research data (raw + derived + documentation)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"

git push
```

## Step 7: Verify CI/CD

Check that automated validation runs:

```bash
# View CI/CD status
gh run list --limit 3

# Or view in browser
gh repo view --web
```

GitHub Actions will automatically:
1. Build Docker container from scratch
2. Validate package dependencies (uses shell-based validation)
3. Run unit tests
4. Report results

**Note**: CI/CD now uses pure shell validation (`make check-renv`) which doesn't require R on the host!

## Step 8: Verify Report Rendering CI/CD (Optional)

GitHub Actions is already configured to automatically render your paper on every
push. The default workflow renders `analysis/paper/paper.Rmd`.

Verify the configuration:

```bash
# Check the render-paper workflow (should already be correct)
cat .github/workflows/render-paper.yml | grep "rmarkdown::render"

# Expected output:
# compendium-env Rscript -e 'rmarkdown::render("analysis/paper/paper.Rmd")'
```

If you used a different filename, update the workflow:
```bash
vim .github/workflows/render-paper.yml
# Change the render command to match your paper filename
git add .github/workflows/render-paper.yml
git commit -m "Update CI/CD render path"
git push
```

**What this enables:**
- Automatic report rendering on every push to main
- Fresh computational environment for each render
- Downloadable PDF/HTML artifacts
- Verification that analysis is fully reproducible

# What You Built

## All 5 Reproducibility Levels

**Level 1**: Basic R Project âœ— (skipped for reproducibility)

**Level 2**: renv âœ“
- `renv.lock` records exact package versions
- `renv::snapshot()` captured dependencies

**Level 3**: Docker âœ“
- Dockerfile specifies complete environment
- Anyone can reproduce with `make r` (launches image for current Dockerfile)

**Level 4**: Unit Testing âœ“
- Tests validate plot creation
- Tests verify missing data handling
- Tests confirm species included

**Level 5**: CI/CD âœ“
- GitHub Actions runs on every push
- Automated test suite execution
- Dependency validation

## The Five Pillars of Reproducibility

Your analysis is now fully reproducible thanks to five essential components that represent the necessary and sufficient elements for complete reproducibility:

### 1. Dockerfile - Computational Environment
- **R version**: Specifies exact R version (4.4.0)
- **System dependencies**: All system libraries (libcurl, libssl, etc.)
- **Base image**: Foundation for consistent environment
- **Environment variables**: Locale and system settings

```dockerfile
FROM rocker/verse:4.4.0

# Set locale for reproducible sorting/formatting
ENV LANG=en_US.UTF-8
ENV LC_ALL=en_US.UTF-8

RUN apt-get update && apt-get install -y libcurl4-openssl-dev
```

### 2. renv.lock - Exact Package Versions
- **Every R package**: Exact version for all packages used
- **Complete dependency tree**: All package dependencies
- **Source repositories**: CRAN, Bioconductor, GitHub

```json
{
  "tidyverse": {
    "Version": "2.0.0",
    "Repository": "CRAN"
  }
}
```

### 3. .Rprofile - R Session Configuration
- **Critical R options**: Version controlled and copied into Docker
- **Monitored settings**: `stringsAsFactors`, `contrasts`, `na.action`, `digits`, `OutDec`
- **Automatic checking**: `check_rprofile_options.R` alerts on changes

```r
# .Rprofile (version controlled)
options(
  stringsAsFactors = FALSE,
  digits = 7,
  OutDec = "."
)

# Activate renv for package management
source("renv/activate.R")
```

**Important**: ZZCOLLAB monitors critical `.Rprofile` options that affect analysis behavior. The `check_rprofile_options.R` script runs in CI/CD to catch changes that could alter results.

### 4. Analysis Source Code - Computational Logic
- **Analysis scripts**: `analysis/scripts/01_analysis.R`
- **Reusable functions**: `R/bill_analysis.R`
- **Reports**: `analysis/paper/paper.Rmd`
- **Tests**: `tests/testthat/test-bill_analysis.R`
- **Random seeds**: Set explicitly in code for stochastic analyses

```r
# Example: Set seed for reproducible random processes
set.seed(42)
model <- randomForest(species ~ ., data = penguins)
```

### 5. Research Data - Empirical Foundation
- **Raw data**: `analysis/data/raw_data/` (original, unmodified files)
- **Derived data**: `analysis/data/derived_data/` (processed, analysis-ready)
- **Data documentation**: `data/README.md` (data dictionary, provenance, processing lineage)
- **Quality assurance**: Validation checks, known issues, outlier documentation

```
analysis/data/
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ penguins_raw.csv      # Original data (read-only)
â”‚   â””â”€â”€ README.md             # Data provenance and collection methods
â””â”€â”€ derived_data/
    â”œâ”€â”€ penguins_clean.csv    # Processed data
    â””â”€â”€ README.md             # Processing steps and transformations
```

**Critical insight**: Without the original research data, perfect computational reproducibility is meaningless. The analysis code operates on data, and that data must be preserved, documented, and version-controlled alongside the computational environment.

### Why Environment Variables Matter

Environment variables affect computational results silently and are critical for reproducibility:

**Locale settings** (`LANG`, `LC_ALL`):
- Control string sorting order (e.g., "Ã…land" sorts differently in US vs Swedish locales)
- Affect number parsing (comma vs period as decimal separator)
- Impact factor level ordering in statistical models

**Timezone** (`TZ`):
- Affects date-time arithmetic and aggregation
- Eliminates daylight saving time complications
- Ensures consistent temporal analyses across team members

**Parallel processing** (`OMP_NUM_THREADS`):
- Controls thread count for deterministic computation
- Single-threaded (`=1`) ensures reproducible results
- Multi-threaded can produce different results each run due to floating-point order

```dockerfile
# Environment variables set in Dockerfile
ENV LANG=en_US.UTF-8        # Standardize locale
ENV LC_ALL=en_US.UTF-8      # Override all locale categories
ENV TZ=UTC                  # No daylight saving time
ENV OMP_NUM_THREADS=1       # Deterministic single-threaded execution
```

**Real-world impact**: Two researchers running identical code in different locales or timezones can produce different results without explicit environment variable specification.

### How They Work Together:

```bash
# Someone reproduces your analysis 5 years from now:
git clone https://github.com/rgt47/penguin-bills.git
cd penguin-bills

# 1. Dockerfile builds environment (R version + system deps)
docker build -t penguin-env .
docker run -it -v $(pwd):/project penguin-env

# 2. renv.lock installs exact package versions
renv::restore()

# 3. Source code runs the analysis
source("analysis/scripts/01_analysis.R")
rmarkdown::render("analysis/paper/paper.Rmd")

# Result: Identical output to your original analysis
```

**Key Insight**:
- **Dockerfile** = Environment foundation (R version, system libraries, environment variables)
- **renv.lock** = Exact packages (source of truth for R dependencies)
- **.Rprofile** = R session settings (critical options that affect behavior)
- **Source Code** = Analytical logic (what actually runs, including random seeds)
- **Research Data** = Empirical observations (what analyses operate on)

All five pillars are version controlled in Git, representing the necessary and sufficient components for complete reproducibility. Without any one pillar, independent reproduction of the analysis becomes impossible.

### Reproducibility Checklist

When preparing analysis for publication or sharing, verify all five pillars.

For comprehensive best practices, troubleshooting, and team collaboration
patterns, see **docs/REPRODUCIBILITY_BEST_PRACTICES.md**.

**Pillar 1 - Dockerfile**:
- âœ… R version explicitly specified
- âœ… System dependencies documented
- âœ… Environment variables set (LANG, LC_ALL, TZ, OMP_NUM_THREADS)

**Pillar 2 - renv.lock**:
- âœ… All packages from all contributors included
- âœ… Exact versions with repository sources
- âœ… Committed to version control

**Pillar 3 - .Rprofile**:
- âœ… Critical options version controlled
- âœ… Monitored by check_rprofile_options.R in CI/CD
- âœ… Copied into Docker container

**Pillar 4 - Source Code**:
- âœ… Random seeds explicit for all stochastic operations
- âœ… Analysis scripts, functions, tests all committed
- âœ… Clear computational workflow documented

**Pillar 5 - Research Data**:
- âœ… Raw data preserved (read-only, original files)
- âœ… Data documentation complete (data dictionary, provenance)
- âœ… Processing scripts link raw to derived data
- âœ… Data quality notes documented

## Project Structure

```
penguin-bills/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ r-package.yml        # Automated CI/CD validation
â”œâ”€â”€ R/
â”‚   â””â”€â”€ bill_analysis.R          # Tested function
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw_data/            # Original, unmodified data
â”‚   â”‚   â”œâ”€â”€ derived_data/        # Processed analysis-ready data
â”‚   â”‚   â””â”€â”€ README.md            # Data dictionary and documentation
â”‚   â”œâ”€â”€ paper/
â”‚   â”‚   â”œâ”€â”€ paper.Rmd            # Analysis report (reproducible)
â”‚   â”‚   â””â”€â”€ paper.html           # Rendered output
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â””â”€â”€ bill_scatter.png     # Generated plot
â”‚   â””â”€â”€ tables/                  # Generated tables
â”œâ”€â”€ data/                        # R package data (optional, for package distribution)
â”œâ”€â”€ scripts/                     # Working scripts and exploration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ testthat/
â”‚       â””â”€â”€ test-bill_analysis.R # Unit tests
â”œâ”€â”€ man/                         # Generated function documentation
â”œâ”€â”€ vignettes/                   # Package tutorials
â”œâ”€â”€ docs/                        # Project documentation
â”œâ”€â”€ renv/                        # renv package cache
â”œâ”€â”€ .Rbuildignore                # Files excluded from package build
â”œâ”€â”€ .gitignore                   # Git exclusions
â”œâ”€â”€ DESCRIPTION                  # Project metadata and dependencies
â”œâ”€â”€ NAMESPACE                    # Function exports (managed by roxygen2)
â”œâ”€â”€ LICENSE                      # GPL-3 license
â”œâ”€â”€ Makefile                     # Development workflow automation
â”œâ”€â”€ Dockerfile                   # Environment specification
â”œâ”€â”€ renv.lock                    # Exact package versions
â””â”€â”€ penguin-bills.Rproj          # RStudio project file
```

## Verification Checklist

- âœ“ Docker image built locally
- âœ“ GitHub repository created (private)
- âœ“ Analysis produces scatter plot
- âœ“ Unit tests pass locally
- âœ“ Dependencies captured in renv.lock
- âœ“ CI/CD validates on every push
- âœ“ Complete reproducibility achieved

# Next Steps

## Extend Your Analysis

```{r eval=FALSE}
# Add more plots
create_flipper_plot <- function(data = penguins) {
  # Flipper length vs body mass
}

# Add statistical analysis
fit_bill_model <- function(data) {
  lm(bill_depth_mm ~ bill_length_mm * species, data = data)
}

# Write more tests
test_that("model includes species interaction", {
  model <- fit_bill_model(penguins)
  expect_true("bill_length_mm:species" %in% names(coef(model)))
})
```

## Collaborate

### Team Lead Workflow

Create a team project and share the Docker image:

```bash
# Create team project structure with profile
mkdir study && cd study
zzcollab -t genomicslab -p study -r bioinformatics

# Customize Dockerfile if needed (optional)
vim Dockerfile

# Build team Docker image
make docker-build

# Push team image to Docker Hub
make docker-push-team

# Commit and push project structure to GitHub
git add .
git commit -m "Initial project setup with bioinformatics profile"
git push -u origin main

# Share with collaborators
gh repo edit --add-collaborator username
```

**Alternative workflows:**

```bash
# Option 2: Custom composition with base image and bundles
mkdir study && cd study
zzcollab -t genomicslab -p study -b bioconductor/bioconductor_docker -l bioinfo -k bioinfo
make docker-build
make docker-push-team
git add .
git commit -m "Initial project setup with custom bioinfo composition"
git push -u origin main
```

**Team Lead Responsibilities:**
1. Creates complete project structure with chosen Docker profile
2. Builds team Docker image with all necessary dependencies
3. Pushes team image to Docker Hub for team members to access
4. Commits and pushes project structure to GitHub
5. Adds collaborators to GitHub repository

**Profile Selection for Teams:**
- Team lead chooses Docker profile with `-r` flag (bioinformatics, geospatial, etc.)
- All team members get identical Docker environment via team image
- Team members add packages dynamically using `install.packages()` as needed

### Team Member Workflow

Clone and join the team project:

```bash
# Clone the repository
git clone https://github.com/genomicslab/study.git
cd study

# Pull and use team Docker image
zzcollab --use-team-image

# Enter development environment
make r
```

**What happens for team members:**
1. `--use-team-image` flag pulls team image from Docker Hub (e.g., `genomicslab/study:latest`)
2. Team image has the Docker profile chosen by team lead (e.g., bioinformatics)
3. Uses the exact same Docker environment as team lead
4. Add packages as needed using `install.packages()` inside the container
5. Exit container - packages automatically captured in renv.lock

**Environment Consistency:**
- **Docker environment**: Identical for all team members (from team image via `--use-team-image`)
- **Package management**: Add packages dynamically with `install.packages()` as needed
- **Automatic capture**: Packages tracked automatically when you exit containers

### Adding Packages to Team Environment

Team members can add additional R packages to their personal Docker layer:

```bash
# Add modeling packages on top of team image
zzcollab -t genomicslab -p study -k modeling

# This installs modeling packages in your personal Docker layer
# Team image remains unchanged
```

**Team member restrictions:**
- âœ— Cannot change Docker profile (use `-r`)
- âœ— Cannot change base image (use `-b`)
- âœ— Cannot add system libraries (use `-l`)
- âœ“ Can add R packages to Docker (use `-k`)
- âœ“ Can add packages to renv.lock (use `install.packages()` in container, auto-captured on exit)

## Publish Results

```bash
# Create manuscript
mkdir -p analysis/paper
# Add paper.Rmd with analysis

# Render paper automatically via CI/CD
# (already configured in .github/workflows/render-paper.yml)
```

# Troubleshooting

## GitHub Repository Creation Fails

```bash
# Create repository manually if needed
cd penguin-bills
gh repo create rgt47/penguin-bills --private --source=. --remote=origin --push
```

## Docker Build Errors

```bash
# Check Docker daemon running
docker info

# Clean rebuild (uses config defaults)
docker system prune
rm -rf penguin-bills  # Remove failed directory

# Retry project creation
mkdir penguin-bills && cd penguin-bills
zzcollab
make docker-build
```

## Test Failures

```bash
# Run tests with verbose output
make r
devtools::test()  # See detailed failure messages

# Check test coverage
covr::package_coverage()
```

## CI/CD Failures

```bash
# View workflow logs
gh run view --log

# Validate locally first
Rscript validate_package_environment.R --quiet --fail-on-issues
make docker-test
```

# Customizing Profiles

## View Available Profiles

```bash
# List all predefined profiles
zzcollab --list-profiles

# List available library bundles (system dependencies)
zzcollab --list-libs

# List available package bundles (R packages)
zzcollab --list-pkgs
```

**14+ Available Profiles:**
- **Standard Research (6)**: minimal, analysis, modeling, publishing, shiny, shiny_verse
- **Specialized Domains (2)**: bioinformatics, geospatial
- **Lightweight Alpine (3)**: alpine_minimal, alpine_analysis, hpc_alpine
- **R-Hub Testing (3)**: rhub_ubuntu, rhub_fedora, rhub_windows

## Using Profiles

```bash
# Method 1: Use -r flag for profile selection (most common)
mkdir my-project && cd my-project
zzcollab -r bioinformatics
make docker-build

# Method 2: Use -r flag for different profile
mkdir spatial-analysis && cd spatial-analysis
zzcollab -r geospatial
make docker-build

# Method 3: Compose custom profile with bundles
mkdir custom-project && cd custom-project
zzcollab -b rocker/r-ver -l geospatial -k geospatial
make docker-build

# Method 4: Mix and match components
mkdir bioinfo-project && cd bioinfo-project
zzcollab -b bioconductor/bioconductor_docker -l bioinfo -k modeling
make docker-build
```

## Profile System Components

The profile system has two independent layers:

### Docker Environment Layer (Team/Shared)

**Profiles** (`-r` flag):
- Complete Docker environment shortcuts (14+ predefined)
- Combines base image + system libraries + R packages
- Examples: `bioinformatics`, `geospatial`, `alpine_minimal`
- Shared across all team members
- Specify via: `zzcollab -r bioinformatics`

**Library Bundles** (`-l` flag):
- System dependencies (apt-get/apk packages)
- Examples: `minimal`, `geospatial`, `bioinfo`, `modeling`, `publishing`, `alpine`
- Required for R packages needing system libraries (GDAL, PROJ, etc.)
- Specify via: `zzcollab -l geospatial`

**Package Bundles** (`-k` flag):
- Pre-installed R packages in Docker image
- Examples: `essential`, `tidyverse`, `modeling`, `bioinfo`, `geospatial`, `publishing`, `shiny`
- Baked into Docker image for fast container startup
- Specify via: `zzcollab -k modeling`

**Package Management:**
- Packages are added dynamically using standard `install.packages()` inside containers
- For GitHub packages: `install.packages("remotes")` then `remotes::install_github("user/package")`
- **Auto-restore**: Missing packages installed automatically when R starts (no manual restore needed)
- **Auto-snapshot**: Packages captured in renv.lock when container exits (no manual snapshot needed)
- No pre-configured modes - just install what you need when you need it
- The renv.lock file accumulates packages from all team members automatically
- This provides maximum flexibility while maintaining reproducibility

# Summary

You created a fully reproducible analysis with:

- **Environment consistency**: Docker ensures identical R, packages, system libraries
- **Computational correctness**: Unit tests validate analytical logic
- **Automated verification**: CI/CD catches errors on every commit
- **Private collaboration**: GitHub private repo ready for team members
- **Professional workflow**: Industry-standard practices for research software

This is the gold standard for reproducible computational research. All 5 levels
working together ensure your analysis is not only reproducible, but also
correct and maintainable.

## Understanding the Two-Layer Architecture

ZZCOLLAB uses a two-layer package management system for maximum flexibility:

### Layer 1: Docker Environment (Team/Shared)
- **Controlled by**: `-r` (profile), `-b` (base image), `-l` (libs), `-k` (pkgs)
- **Purpose**: Pre-installed packages in Docker image
- **Shared**: All team members get identical Docker environment
- **Examples**: Bioinformatics profile, geospatial libraries, tidyverse packages
- **When to use**: For packages everyone on team needs

### Layer 2: Dynamic Package Installation (Personal)
- **Controlled by**: `install.packages()` inside containers (auto-captured on exit)
- **Purpose**: Add packages as needed for specific analyses
- **Flexible**: Each team member can add packages independently
- **Collaborative**: renv.lock accumulates packages from all contributors automatically
- **When to use**: Whenever you need a package for your analysis

### Practical Example

Team lead sets up bioinformatics environment:
```bash
mkdir study && cd study
zzcollab -t genomicslab -p study -r bioinformatics
make docker-build
make docker-push-team
git add .
git commit -m "Initial project setup"
git push -u origin main
```

Team members join and add packages as needed:
```bash
# Clone repository
git clone https://github.com/genomicslab/study.git
cd study

# Pull team image
zzcollab --use-team-image

# Enter container and add packages
make r
# Auto-restore runs if any packages missing

# Inside container - Alice adds modeling packages
install.packages("tidymodels")  # Standard R command
exit  # Auto-snapshot captures tidymodels in renv.lock!

# Bob adds geospatial packages (in his container)
make r  # Auto-restore installs tidymodels (from Alice)
install.packages("sf")  # Standard R command
exit  # Auto-snapshot captures both tidymodels + sf!
```

**Result**:
- Both get identical Bioconductor Docker environment (Layer 1) via team image
- Each adds packages dynamically as needed (Layer 2)
- Final renv.lock contains packages from both Alice and Bob

### What Happens to Final renv.lock?

**Important**: When multiple team members work together, the final `renv.lock` file contains packages from ALL contributors:

```bash
# Alice adds modeling packages
make r

# Inside container:
install.packages("tidymodels")
# Exit container (auto-snapshot happens here!)
exit

# Validation runs automatically after container exit
# Then timestamp is restored for accurate git history

# STEP 2: Run tests to ensure nothing broke
make docker-test

# STEP 3: Commit if validation passed
git add renv.lock DESCRIPTION
git commit -m "Add tidymodels for modeling workflow"
git push

# Bob adds geospatial packages
git pull  # Gets Alice's changes
make r

# Inside container:
install.packages("sf")
# Exit container (auto-snapshot, renv.lock now has tidymodels + sf)
exit

# Validation runs automatically after container exit

# STEP 2: Test functionality
make docker-test

# STEP 3: Commit
git add renv.lock DESCRIPTION
git commit -m "Add sf for geospatial analysis"
git push

# Final renv.lock contains BOTH Alice's and Bob's packages
```

**For reproducibility, renv.lock is the source of truth, NOT the Docker image.**

When someone reproduces the analysis:
```bash
# Pulls repository with complete renv.lock
git clone https://github.com/genomicslab/study.git
cd study

# Uses team Docker image (has base bioinformatics packages)
zzcollab --use-team-image
make r
# Auto-restore runs automatically on R startup!
# Installs tidymodels + sf + ALL dependencies (no manual restore needed)
```

**Key Insight**: The Docker image provides the foundation (system dependencies, R version, base packages for performance), but `renv.lock` ensures complete reproducibility by specifying exact versions of ALL packages needed for the analysis.

For more details on each level, see `vignette("reproducibility-layers")`.
