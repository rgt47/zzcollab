---
title: "Why Reproducibility Matters: The Case for Docker and renv in Data Analysis"
author: "ZZCOLLAB Project"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Why Reproducibility Matters: The Case for Docker and renv in Data Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# The Reproducibility Crisis in Science

The scientific community faces a reproducibility crisis. A landmark study found that only 39% of published psychology experiments could be successfully replicated [@opensciencecollaboration2015]. In cancer biology, the situation is even more dire—researchers could reproduce only 11% of landmark studies [@begley2012]. While these crises extend beyond computational reproducibility, the computational components of modern research add unique challenges that are entirely preventable with proper tooling.

As @peng2011 argues, reproducibility represents a **minimum standard** for judging scientific claims when full independent replication is impractical. For computational research, this standard is achievable: given the same data and code, any researcher should obtain identical numerical results. Yet this seemingly simple requirement remains elusive in practice.

## Why Computational Reproducibility Fails

The "works on my machine" problem plagues computational research. @stodden2018 surveyed 204 Science articles and found that 56% of authors who said they would share materials failed to do so when asked. Even when materials are shared, reproduction often fails due to undocumented dependencies, missing software versions, or implicit environmental assumptions.

The root causes are systematic:

1. **Dependency hell**: Modern analyses depend on dozens to hundreds of software packages, each with their own version histories and breaking changes [@wickham2015]
2. **Environmental variation**: System libraries, locales, and R versions all affect computational results in subtle ways [@gentleman2007]
3. **Implicit configurations**: `.Rprofile` settings and other hidden configurations create invisible dependencies [@marwick2018]
4. **Temporal drift**: Software ecosystems evolve continuously; code that works today may break tomorrow without any changes to the analysis itself

## The Cost of Non-Reproducibility

### Personal Costs

Every researcher has experienced the frustration of returning to their own analysis months later, unable to reproduce their previous results. @wilson2014 surveyed scientists and found that 90% reported spending significant time dealing with "code rot"—the decay of computational methods over time. This represents real opportunity costs: time spent debugging environment issues is time not spent doing science.

### Collaborative Costs

Non-reproducible workflows create friction in collaborative research. @ram2013 documents how version conflicts and environment mismatches slow team productivity. When each team member maintains a different computational environment, integrating contributions becomes a manual, error-prone process.

### Scientific Costs

Most seriously, reproducibility failures undermine scientific validity. @ioannidis2005 argues that most published research findings are false, with computational errors contributing to this problem. @nuijten2016 found that 50% of psychology papers contain at least one statistical reporting error, many traceable to computational issues.

# The Solution: Comprehensive Environmental Control

The solution requires controlling **all** sources of computational variation. Neither documenting dependencies nor containerization alone suffices—both are necessary for complete reproducibility.

## The Five Pillars of Computational Reproducibility

Building on the research compendium framework of @marwick2018, we identify five necessary and sufficient components:

### 1. Computational Environment (Dockerfile)

Docker containers provide bit-for-bit identical computational environments [@boettiger2015]. A Dockerfile specifies:

- R version (e.g., 4.4.0)
- System libraries (BLAS/LAPACK, libcurl, libxml2)
- Operating system (Ubuntu 24.04)
- Environment variables (locale, timezone, thread count)

**Why it matters**: @buckheit1995 established that "an article about computational science... is **not** the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures."

### 2. R Package Ecosystem (renv.lock)

The `renv` package [@ushey2020] creates isolated, portable R environments by recording exact package versions and their dependencies. This addresses the package versioning problem documented by @trisovic2022, who found that 74% of R scripts fail to run after just 2-3 years due to package updates.

### 3. R Session Configuration (.Rprofile)

Session options like `stringsAsFactors`, `contrasts`, and `OutDec` silently affect computational results [@rcoreteam2020]. These settings are automatically loaded before code execution, creating invisible dependencies that @gentleman2007 identifies as a major source of irreproducibility.

### 4. Analysis Code

The computational logic itself must be version controlled and documented. @wilson2017 provides best practices for scientific computing, emphasizing that code is a research product deserving the same care as manuscripts.

### 5. Research Data

@white2013 establish principles for data archiving, emphasizing that data without processing code provides incomplete reproducibility. The research compendium model [@marwick2018] integrates data, code, and documentation into a single reproducible unit.

# Real-World Failure Modes

Understanding how reproducibility fails in practice motivates the need for comprehensive tooling.

## Case Study 1: The stringsAsFactors Debacle

R 4.0.0 (released April 2020) changed the default behavior of `read.csv()` and `data.frame()`, switching `stringsAsFactors` from `TRUE` to `FALSE` [@rcoreteam2020]. This single change broke thousands of analysis scripts worldwide.

```{r stringsAsFactors-example}
# R 3.6.3 (pre-April 2020):
data <- read.csv("patients.csv")
class(data$treatment)  # Returns "factor"
model <- lm(recovery ~ treatment, data)  # Works correctly

# R 4.0.0+ (post-April 2020):
data <- read.csv("patients.csv")  # SAME CODE
class(data$treatment)  # Returns "character" - DIFFERENT!
model <- lm(recovery ~ treatment, data)  # ERROR: variable lengths differ
```

A researcher who published results using R 3.6.3 would find that collaborators using R 4.0.0+ could not reproduce their analysis **using identical code and data**. This violates the basic promise of computational reproducibility.

**Impact documented by @trisovic2022**: In their survey of 9,000+ R scripts from Harvard Dataverse, 74% failed to run after 2-3 years, with R version incompatibilities being a primary cause.

## Case Study 2: Random Number Generator Changes

R has changed its random number generator multiple times (versions 3.6.0 and 4.4.0), each time breaking the reproducibility of stochastic analyses [@rcoreteam2019].

```{r rng-example}
# R 4.3.0:
set.seed(123)
sample(1:10)  # [3, 7, 1, 9, 5, 2, 8, 4, 6, 10]

# R 4.4.0 (changed RNG):
set.seed(123)  # SAME SEED
sample(1:10)  # [5, 2, 8, 4, 1, 9, 3, 7, 6, 10]  # DIFFERENT SEQUENCE
```

This affects bootstrapping, cross-validation, permutation tests, and all simulation-based inference. @patil2016 documents how seemingly minor computational details like RNG algorithms critically affect reproducibility in simulation studies.

## Case Study 3: Numerical Precision and Linear Algebra Libraries

Different BLAS (Basic Linear Algebra Subprograms) implementations produce numerically equivalent but not identical results due to precision differences and different optimization strategies [@wang2021].

```{r blas-example}
# System with OpenBLAS:
set.seed(123)
pca <- prcomp(data, scale = TRUE)
loadings1 <- pca$rotation[, 1:3]

# System with Intel MKL:
set.seed(123)  # SAME SEED
pca <- prcomp(data, scale = TRUE)
loadings2 <- pca$rotation[, 1:3]

# loadings1 ≈ loadings2, but NOT identical
# all.equal(loadings1, loadings2)  # FALSE
# Downstream analyses differ
```

This affects PCA, eigendecomposition, regression, and essentially all matrix operations central to modern statistics and machine learning.

# Why Both Docker AND renv Are Necessary

A common misconception is that dependency management (renv) or containerization (Docker) alone provides sufficient reproducibility. This is false.

## What renv Alone Cannot Fix

renv provides comprehensive R package version control but cannot control:

- R version itself (critical for stringsAsFactors, RNG, and other breaking changes)
- System libraries (BLAS, libcurl, libxml2, OpenSSL versions)
- Operating system (Ubuntu vs. macOS numerical precision differences)
- Locale settings (affects text sorting, number formatting, factor ordering)
- Timezone (affects datetime calculations, especially with daylight saving time)

As @gentleman2007 note: "Reproducible research requires that the entire computational environment be captured." Package versions alone represent only part of this environment.

## What Docker Alone Cannot Fix

Docker provides bit-for-bit identical system environments but cannot control:

- R package versions (users can install arbitrary versions after container launch)
- Package drift (CRAN updates packages continuously)
- Developer intentions (which packages to install for a given analysis)

@boettiger2015 acknowledges this limitation, recommending Docker + package version locking for complete reproducibility.

## The Complete Solution: Docker + renv + .Rprofile

@marwick2018 provide the theoretical framework for this integrated approach in their research compendium model. The combination provides:

1. **System-level control** (Docker): R version, system libraries, OS, locale, timezone
2. **Package-level control** (renv): Exact R package versions with complete dependency trees
3. **Session-level control** (.Rprofile): R options that affect computation but are invisible in code

This three-layer approach addresses all documented sources of computational variation in R-based research.

# The Effort is Worth It: Benefits Outweigh Costs

## Reduced Time Costs

While implementing reproducible workflows requires upfront investment, @ram2013 found that this pays dividends through:

- Faster onboarding of new collaborators
- Elimination of "works on my machine" debugging
- Ability to return to old projects without reconstruction effort
- Reduced time spent on reviewer requests to verify results

@wilson2014 surveyed scientists and found that those using reproducible practices spent **less** total time on computational work despite the initial setup cost.

## Enhanced Scientific Validity

@peng2011 argues that reproducibility should be the minimum standard for publication. Journals increasingly require it: *Nature*, *Science*, and *PLOS* now encourage or mandate sharing of computational environments [@nature2019].

More importantly, reproducible practices reduce errors. @stodden2013 found that researchers using version control and environment management caught more bugs in their own code.

## Career Benefits

Reproducible research enhances scientific impact. @vandewalle2009 found that papers with publicly available code and data receive higher citation rates. @piwowar2013 demonstrated that sharing data increases citation rates by 9% on average.

@mckiernan2016 show that open research practices—including computational reproducibility—correlate with career advancement, not hindrance.

## Collaborative Benefits

Modern science is increasingly collaborative. @wuchty2007 document that team-authored papers have increased from 17.5% in 1955 to 51.5% in 2000, with continued growth. Reproducible workflows reduce friction in collaboration by ensuring all team members work in identical computational environments.

# Practical Implementation with ZZCOLLAB

ZZCOLLAB implements the complete Docker + renv + .Rprofile approach with minimal friction.

## Fast Builds Enable Git-Based Distribution

A critical insight: with RSPM (RStudio Package Manager) binary packages, Docker images build in 3-4 minutes instead of 30-60 minutes [@rstudio2023]. This changes the distribution model:

```bash
# Team member joins project
git clone https://github.com/lab/study.git
cd study
make docker-build    # 3-4 minutes
make docker-run      # Start working
```

No Docker Hub required—team members build from the Dockerfile in the repository, ensuring perfect reproducibility with minimal wait time.

## Automatic Snapshot on Exit

ZZCOLLAB implements an automatic snapshot-on-exit architecture that eliminates manual `renv::snapshot()` commands:

```bash
make docker-run
# Inside container:
renv::install("ggplot2")  # Add packages as needed
exit                      # Automatically snapshots renv.lock
```

This addresses @ram2013's finding that manual processes create reproducibility failures.

## Volume Mounts Enable Iterative Development

Docker containers are often criticized as inflexible for active development. ZZCOLLAB solves this by mounting the project directory as a volume:

```bash
docker run -v $(pwd):/home/analyst/project ...
```

Changes inside the container persist to the host, providing normal iterative development workflows within a reproducible environment.

# Comparison with Alternatives

## Conda/Mamba

Conda provides cross-language package management but has limitations for R:

- R package versions lag behind CRAN by months [@anaconda2023]
- Not all CRAN packages available
- Mixing conda-forge and CRAN causes conflicts
- Slower than RSPM binaries

**When to use**: Python-centric workflows with some R components.

## Binder/MyBinder

Binder provides cloud-based reproducible environments but:

- Requires internet connection
- Limited computational resources
- Cannot use local data
- Session timeouts

**When to use**: Teaching, demonstrations, lightweight analyses.

## Manual Documentation

Comprehensive installation instructions fail because:

- Nobody follows instructions perfectly [@stodden2018]
- Hours to set up
- Platform-specific variations
- Not actually reproducible in practice

# Conclusion

The reproducibility crisis in science demands solutions. For computational research, the technical solutions exist: Docker for environmental control, renv for package management, and version-controlled .Rprofile for session configuration.

The evidence shows that reproducible practices:

1. **Reduce long-term time costs** [@wilson2014]
2. **Improve scientific validity** [@stodden2013]
3. **Enhance career outcomes** [@mckiernan2016]
4. **Increase research impact** [@piwowar2013]
5. **Enable effective collaboration** [@ram2013]

The upfront cost is modest—3-4 minutes for Docker builds with RSPM, plus initial learning—while the benefits compound over time.

As @buckheit1995 stated: "An article about computational science in a scientific publication is **not the scholarship itself**, it is merely **advertising** of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures."

Docker + renv + .Rprofile represents the current best practice for achieving this standard. ZZCOLLAB implements this complete solution with minimal friction, making reproducibility the path of least resistance rather than a burden.

# References

::: {#refs}
:::

# Appendix: Setting Up Reproducible Workflows

## Quick Start

```bash
# Install ZZCOLLAB
git clone https://github.com/rgt47/zzcollab.git
cd zzcollab && ./install.sh

# Create a reproducible project
mkdir my-analysis && cd my-analysis
zzcollab -r analysis -d ~/dotfiles

# Build environment (3-4 minutes with RSPM)
make docker-build

# Start working
make docker-run
```

## Daily Workflow

```bash
# Enter reproducible environment
make docker-run

# Inside container:
devtools::load_all()                    # Load your R package
source("analysis/scripts/01_clean.R")   # Run analysis
renv::install("tidymodels")             # Add new packages
exit                                    # Auto-snapshot on exit

# Commit and share
git add . && git commit -m "Analysis update"
git push
```

## Sharing with Collaborators

```bash
# Collaborator clones and builds
git clone https://github.com/you/project.git
cd project
make docker-build    # 3-4 minutes
make docker-run      # Identical environment
```

## For More Information

- ZZCOLLAB documentation: `docs/` directory
- Docker guide: `docs/DOCKER_ARCHITECTURE.md`
- Testing guide: `docs/TESTING_GUIDE.md`
- Configuration: `docs/CONFIGURATION.md`
